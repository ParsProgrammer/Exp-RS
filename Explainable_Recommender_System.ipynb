{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# uncomment the last two cell to clone the data and enable the GPU\n",
        "#! git clone https://github.com/ParsProgrammer/Exp-RS.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#cd Exp-RS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrzVYQNxJm_W",
        "outputId": "8f98d417-6c2d-47ff-a041-84238103eba9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.9 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (2.9.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (3.7.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (0.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.14.1)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (2.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.16.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (61.2.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (2.9.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.50.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (0.26.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.1.0)\n",
            "Requirement already satisfied: packaging in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (21.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (14.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (1.12)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (4.1.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-gpu==2.9) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow-gpu==2.9) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.28.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.2.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.9.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from packaging->tensorflow-gpu==2.9) (3.0.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (4.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow-gpu==2.9) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tensorflow-text in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (2.9.0)\n",
            "Requirement already satisfied: tensorflow<2.10,>=2.9.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-text) (2.9.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow-text) (0.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (14.0.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (2.9.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (2.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.7.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (4.1.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.26.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (2.9.0)\n",
            "Requirement already satisfied: setuptools in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (61.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.50.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.12)\n",
            "Requirement already satisfied: packaging in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (21.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.2.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.9.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.28.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.26.11)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2022.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow-gpu==2.9\n",
        "%pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BTOV54y-ckzq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: matplotlib in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (3.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (4.34.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (9.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1u4wpNQOEUc9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "import gc\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import KFold\n",
        "import datetime \n",
        "import os\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTkARkSuo9PR",
        "outputId": "8548446d-69cd-4727-8b8f-851569dd33d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim==3.8.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (3.8.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from gensim==3.8.3) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from gensim==3.8.3) (1.16.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from gensim==3.8.3) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from gensim==3.8.3) (1.21.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: nltk in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (3.7)\n",
            "Requirement already satisfied: click in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from nltk) (2022.7.25)\n",
            "Requirement already satisfied: joblib in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from click->nltk) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from importlib-metadata->click->nltk) (3.8.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install gensim==3.8.3\n",
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnDuJU4fpMLE",
        "outputId": "ecfd7d10-b929-43ce-cd99-0e2213f4e376"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/mobin/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-25 12:23:08.986793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-25 12:23:09.006762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-25 12:23:09.006933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-25 12:23:09.007617: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-12-25 12:23:09.008838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-25 12:23:09.008984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-25 12:23:09.009105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-25 12:23:09.317887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-25 12:23:09.318042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-25 12:23:09.318156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-12-25 12:23:09.318243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2132 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "else :\n",
        "  print(\"No GPU available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (2.9.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (1.21.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (61.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (2.9.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (2.2.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (1.50.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (3.19.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard) (2.28.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard) (4.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.11)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (4.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: tensorboard_plugin_profile in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (2.11.1)\n",
            "Requirement already satisfied: gviz-api>=1.9.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard_plugin_profile) (1.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard_plugin_profile) (2.2.1)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard_plugin_profile) (3.19.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard_plugin_profile) (61.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from tensorboard_plugin_profile) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard_plugin_profile) (2.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorboard\n",
        "%pip install -U tensorboard_plugin_profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.summarization.summarizer import summarize\n",
        "from gensim.summarization import keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from vaderSentiment) (2.28.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests->vaderSentiment) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests->vaderSentiment) (2.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests->vaderSentiment) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages (from requests->vaderSentiment) (1.26.11)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyypFCrlw2SV"
      },
      "source": [
        "# **Data Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "p7IbgFIxRQb-"
      },
      "outputs": [],
      "source": [
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    name=b'\"verified\": \\\"true\\\",'\n",
        "    l=l.replace(b'\"verified\": true,',bytes(name))\n",
        "    name1=b'\"verified\": \\\"false\\\",'\n",
        "    l=l.replace(b'\"verified\": false,',bytes(name))\n",
        "    yield eval(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "df = getDF('reviews_Grocery_and_Gourmet_Food_5.json.gz')\n",
        "\n",
        "# dataset link\n",
        "# Grocery and Gourmet Food\n",
        "# https://jmcauley.ucsd.edu/data/amazon/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_auxwvSRYsvu"
      },
      "source": [
        "Dataset Schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E9FGFZINRQOn",
        "outputId": "0dad1216-9ec6-4528-fa44-32ab26a94602"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A1VEELTKS8NLZB</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>Just another flavor of Kit Kat but the taste i...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A14R9XMZVJ6INB</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>I bought this on impulse and it comes from Jap...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A27IQHDZFQFNGG</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>Really good. Great gift for any fan of green t...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A31QY5TASILE89</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>I had never had it before, was curious to see ...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A2LWK003FFMCI5</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>I've been looking forward to trying these afte...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151249</th>\n",
              "      <td>A2L6QS8SVHT9RG</td>\n",
              "      <td>B00KCJRVO2</td>\n",
              "      <td>Delicious gluten-free oatmeal: we tried both t...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151250</th>\n",
              "      <td>AFJFXN42RZ3G2</td>\n",
              "      <td>B00KCJRVO2</td>\n",
              "      <td>With the many selections of instant oatmeal ce...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151251</th>\n",
              "      <td>ASEBX8TBYWQWA</td>\n",
              "      <td>B00KCJRVO2</td>\n",
              "      <td>While I usually review CDs and DVDs, as well a...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151252</th>\n",
              "      <td>ANKQGTXHREOI5</td>\n",
              "      <td>B00KCJRVO2</td>\n",
              "      <td>My son and I enjoyed these oatmeal packets.  H...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151253</th>\n",
              "      <td>A2CF66KIQ3RKX3</td>\n",
              "      <td>B00KCJRVO2</td>\n",
              "      <td>I like to eat oatmeal i the mornings. I usuall...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>151254 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                userID      itemID  \\\n",
              "0       A1VEELTKS8NLZB  616719923X   \n",
              "1       A14R9XMZVJ6INB  616719923X   \n",
              "2       A27IQHDZFQFNGG  616719923X   \n",
              "3       A31QY5TASILE89  616719923X   \n",
              "4       A2LWK003FFMCI5  616719923X   \n",
              "...                ...         ...   \n",
              "151249  A2L6QS8SVHT9RG  B00KCJRVO2   \n",
              "151250   AFJFXN42RZ3G2  B00KCJRVO2   \n",
              "151251   ASEBX8TBYWQWA  B00KCJRVO2   \n",
              "151252   ANKQGTXHREOI5  B00KCJRVO2   \n",
              "151253  A2CF66KIQ3RKX3  B00KCJRVO2   \n",
              "\n",
              "                                               reviewText  rating  \n",
              "0       Just another flavor of Kit Kat but the taste i...     4.0  \n",
              "1       I bought this on impulse and it comes from Jap...     3.0  \n",
              "2       Really good. Great gift for any fan of green t...     4.0  \n",
              "3       I had never had it before, was curious to see ...     5.0  \n",
              "4       I've been looking forward to trying these afte...     4.0  \n",
              "...                                                   ...     ...  \n",
              "151249  Delicious gluten-free oatmeal: we tried both t...     4.0  \n",
              "151250  With the many selections of instant oatmeal ce...     4.0  \n",
              "151251  While I usually review CDs and DVDs, as well a...     5.0  \n",
              "151252  My son and I enjoyed these oatmeal packets.  H...     4.0  \n",
              "151253  I like to eat oatmeal i the mornings. I usuall...     4.0  \n",
              "\n",
              "[151254 rows x 4 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.rename(columns={\"reviewerID\": \"userID\", \"asin\": \"itemID\",\"overall\":\"rating\"},inplace=True)\n",
        "df=df[['userID','itemID','reviewText','rating']]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYb9dMRLXjZT"
      },
      "source": [
        "# **Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yY4NJsolgtl"
      },
      "source": [
        "**User Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9kPFQ1GJr_I"
      },
      "source": [
        "determining all unique users with their reviews and ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "l_eSbOjWRQD3",
        "outputId": "63fd7abe-5957-46a2-aa85-8ca3721bdec5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>itemID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A00177463W0XWB16A9O05</td>\n",
              "      <td>[It is a good stand by coffee you can count on...</td>\n",
              "      <td>[B0029XDZIK, B003C4YIFE, B003YUW7EK, B00474OR8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A022899328A0QROR32DCT</td>\n",
              "      <td>[awesome texture for even the gluten eating ea...</td>\n",
              "      <td>[B000EVE3Y4, B001ACMCNU, B003TO9RSU, B003V8QGA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A04309042SDSL8YX2HRR7</td>\n",
              "      <td>[I love roasted garlic &amp; sweet bell peppers. Y...</td>\n",
              "      <td>[B000B6J51I, B000EM6Q34, B000FYXBPW, B003VN74V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A068255029AHTHDXZURNU</td>\n",
              "      <td>[These bars are especially delicious for cocon...</td>\n",
              "      <td>[B000HKFF94, B000K8WVYA, B001FA1K2G, B00474VPY...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A06944662TFWOKKV4GJKX</td>\n",
              "      <td>[UGH!  My stomach has been really killing me l...</td>\n",
              "      <td>[B000CMD63E, B000CQBZPG, B000GFYRHG, B000GZYAR...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14676</th>\n",
              "      <td>AZWRZZAMX90VT</td>\n",
              "      <td>[Very nice. Not spicy, not too salty, lots of ...</td>\n",
              "      <td>[B0007R9L4M, B0007R9L5Q, B000CN7BMA, B000CQ01G...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14677</th>\n",
              "      <td>AZXKAH2DE6C8A</td>\n",
              "      <td>[Could not imagine having such a rich tasting ...</td>\n",
              "      <td>[B0004N14BC, B0005XOVY8, B000EML7DS, B000EOXQR...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14678</th>\n",
              "      <td>AZXON596A1VXC</td>\n",
              "      <td>[I was a bit skeptical when I bought this prod...</td>\n",
              "      <td>[B00113SKZW, B00113ZTVK, B001EO5RCM, B001EO5S0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14679</th>\n",
              "      <td>AZYXC63SS008M</td>\n",
              "      <td>[This is just about the healthiest you can get...</td>\n",
              "      <td>[B000NGNEKY, B0039LVLS2, B0040QCJDG, B0040WCQK...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14680</th>\n",
              "      <td>AZZ5ASC403N74</td>\n",
              "      <td>[Everybody loves homemade spaghetti sauce, but...</td>\n",
              "      <td>[B0035N3ADS, B004FEN3GA, B004U49QU2, B00BIEU5P...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14681 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      userID  \\\n",
              "0      A00177463W0XWB16A9O05   \n",
              "1      A022899328A0QROR32DCT   \n",
              "2      A04309042SDSL8YX2HRR7   \n",
              "3      A068255029AHTHDXZURNU   \n",
              "4      A06944662TFWOKKV4GJKX   \n",
              "...                      ...   \n",
              "14676          AZWRZZAMX90VT   \n",
              "14677          AZXKAH2DE6C8A   \n",
              "14678          AZXON596A1VXC   \n",
              "14679          AZYXC63SS008M   \n",
              "14680          AZZ5ASC403N74   \n",
              "\n",
              "                                              reviewText  \\\n",
              "0      [It is a good stand by coffee you can count on...   \n",
              "1      [awesome texture for even the gluten eating ea...   \n",
              "2      [I love roasted garlic & sweet bell peppers. Y...   \n",
              "3      [These bars are especially delicious for cocon...   \n",
              "4      [UGH!  My stomach has been really killing me l...   \n",
              "...                                                  ...   \n",
              "14676  [Very nice. Not spicy, not too salty, lots of ...   \n",
              "14677  [Could not imagine having such a rich tasting ...   \n",
              "14678  [I was a bit skeptical when I bought this prod...   \n",
              "14679  [This is just about the healthiest you can get...   \n",
              "14680  [Everybody loves homemade spaghetti sauce, but...   \n",
              "\n",
              "                                                  itemID  \n",
              "0      [B0029XDZIK, B003C4YIFE, B003YUW7EK, B00474OR8...  \n",
              "1      [B000EVE3Y4, B001ACMCNU, B003TO9RSU, B003V8QGA...  \n",
              "2      [B000B6J51I, B000EM6Q34, B000FYXBPW, B003VN74V...  \n",
              "3      [B000HKFF94, B000K8WVYA, B001FA1K2G, B00474VPY...  \n",
              "4      [B000CMD63E, B000CQBZPG, B000GFYRHG, B000GZYAR...  \n",
              "...                                                  ...  \n",
              "14676  [B0007R9L4M, B0007R9L5Q, B000CN7BMA, B000CQ01G...  \n",
              "14677  [B0004N14BC, B0005XOVY8, B000EML7DS, B000EOXQR...  \n",
              "14678  [B00113SKZW, B00113ZTVK, B001EO5RCM, B001EO5S0...  \n",
              "14679  [B000NGNEKY, B0039LVLS2, B0040QCJDG, B0040WCQK...  \n",
              "14680  [B0035N3ADS, B004FEN3GA, B004U49QU2, B00BIEU5P...  \n",
              "\n",
              "[14681 rows x 3 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_df=df[['userID','reviewText',\"itemID\"]].groupby('userID')['reviewText','itemID'].apply(lambda x: pd.Series([list(x['reviewText']),list(x['itemID'])],index=['reviewText', 'itemID'])).reset_index()\n",
        "user_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReAbgOc7q3Tp"
      },
      "source": [
        "**Item Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arvKphT7KCTg"
      },
      "source": [
        "determining all unique items with their reviews and ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "qAjW8mmJq9Cj",
        "outputId": "e03f94a0-8923-417b-c9c0-fd8cd33ea676"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mobin/anaconda3/envs/colabenv/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>itemID</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>userID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>616719923X</td>\n",
              "      <td>[Just another flavor of Kit Kat but the taste ...</td>\n",
              "      <td>[A1VEELTKS8NLZB, A14R9XMZVJ6INB, A27IQHDZFQFNG...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9742356831</td>\n",
              "      <td>[This curry paste makes a delicious curry.  I ...</td>\n",
              "      <td>[A23RYWDS884TUL, A945RBQWGZXCK, A1TCSC0YWT82Q0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B00004S1C5</td>\n",
              "      <td>[These dyes create awesome colors for kids cra...</td>\n",
              "      <td>[A1C8NAHYR6Z10F, A14YSMLYLJEMET, A1358PQON9ZAK...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>B0000531B7</td>\n",
              "      <td>[I really enjoy these bars as a quick breakfas...</td>\n",
              "      <td>[ATN5X2PM7OB3K, A2BYV7S1QP2YIG, A2TN9C5E4A0I3F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>B00005344V</td>\n",
              "      <td>[Traditional Medicinals' \"Breathe Easy\" is an ...</td>\n",
              "      <td>[A3EBHHCZO6V2A4, A1P9UMP1XSE6MI, A2F488C4PLWGE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8708</th>\n",
              "      <td>B00JGPG60I</td>\n",
              "      <td>[We switched to this formula 5 days ago and fo...</td>\n",
              "      <td>[A2D7X9N3IV3S7B, A36NUDST4Y5JBA, A2E4R7YISIM4Q...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8709</th>\n",
              "      <td>B00JL6LTMW</td>\n",
              "      <td>[We have enjoyed Larabar's variety of bars for...</td>\n",
              "      <td>[A1QR76SYGTXJN5, A1P2XYD265YE21, A1P9BVW2JB1OV...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8710</th>\n",
              "      <td>B00K00H9I6</td>\n",
              "      <td>[This 100% pure Canadian maple syrup is a Grad...</td>\n",
              "      <td>[A23GFTVIETX7DS, A35W3JQYP0M655, A1UQBFCERIP7V...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8711</th>\n",
              "      <td>B00KC0LGI8</td>\n",
              "      <td>[I followed the directions on the box exactly ...</td>\n",
              "      <td>[A34U4Y40W1GW9I, A1P9BVW2JB1OVL, A3DS0VAXL90E2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8712</th>\n",
              "      <td>B00KCJRVO2</td>\n",
              "      <td>[Usually the label &amp;#34;gluten free&amp;#34; is a ...</td>\n",
              "      <td>[A3H8PA7AG48K33, A2H2I5FY1PUHP1, A3JH18T58CY65...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8713 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          itemID                                         reviewText  \\\n",
              "0     616719923X  [Just another flavor of Kit Kat but the taste ...   \n",
              "1     9742356831  [This curry paste makes a delicious curry.  I ...   \n",
              "2     B00004S1C5  [These dyes create awesome colors for kids cra...   \n",
              "3     B0000531B7  [I really enjoy these bars as a quick breakfas...   \n",
              "4     B00005344V  [Traditional Medicinals' \"Breathe Easy\" is an ...   \n",
              "...          ...                                                ...   \n",
              "8708  B00JGPG60I  [We switched to this formula 5 days ago and fo...   \n",
              "8709  B00JL6LTMW  [We have enjoyed Larabar's variety of bars for...   \n",
              "8710  B00K00H9I6  [This 100% pure Canadian maple syrup is a Grad...   \n",
              "8711  B00KC0LGI8  [I followed the directions on the box exactly ...   \n",
              "8712  B00KCJRVO2  [Usually the label &#34;gluten free&#34; is a ...   \n",
              "\n",
              "                                                 userID  \n",
              "0     [A1VEELTKS8NLZB, A14R9XMZVJ6INB, A27IQHDZFQFNG...  \n",
              "1     [A23RYWDS884TUL, A945RBQWGZXCK, A1TCSC0YWT82Q0...  \n",
              "2     [A1C8NAHYR6Z10F, A14YSMLYLJEMET, A1358PQON9ZAK...  \n",
              "3     [ATN5X2PM7OB3K, A2BYV7S1QP2YIG, A2TN9C5E4A0I3F...  \n",
              "4     [A3EBHHCZO6V2A4, A1P9UMP1XSE6MI, A2F488C4PLWGE...  \n",
              "...                                                 ...  \n",
              "8708  [A2D7X9N3IV3S7B, A36NUDST4Y5JBA, A2E4R7YISIM4Q...  \n",
              "8709  [A1QR76SYGTXJN5, A1P2XYD265YE21, A1P9BVW2JB1OV...  \n",
              "8710  [A23GFTVIETX7DS, A35W3JQYP0M655, A1UQBFCERIP7V...  \n",
              "8711  [A34U4Y40W1GW9I, A1P9BVW2JB1OVL, A3DS0VAXL90E2...  \n",
              "8712  [A3H8PA7AG48K33, A2H2I5FY1PUHP1, A3JH18T58CY65...  \n",
              "\n",
              "[8713 rows x 3 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "item_df=df[['itemID','reviewText','userID']].groupby('itemID')['reviewText','userID'].apply(lambda x: pd.Series([list(x['reviewText']),list(x['userID'])],index=['reviewText', 'userID'])).reset_index()\n",
        "item_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "review num :  151254 user num :  14681 item num :  8713\n"
          ]
        }
      ],
      "source": [
        "print(\"review num : \", df.shape[0] ,\"user num : \" ,user_df.shape[0],\"item num : \",item_df.shape[0] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCekVFHgXtom"
      },
      "source": [
        "##   Ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "cZb_XiBoGHSa",
        "outputId": "f1cb72ea-8d50-467a-d412-92d75a9d167e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    151254.000000\n",
              "mean          4.243042\n",
              "std           1.090003\n",
              "min           1.000000\n",
              "25%           4.000000\n",
              "50%           5.000000\n",
              "75%           5.000000\n",
              "max           5.000000\n",
              "Name: rating, dtype: float64"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df.groupby('rating').size().plot(kind=\"bar\");\n",
        "df['rating'].describe()\n",
        "#histogram of ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxGLa04uYQRt"
      },
      "source": [
        "## Reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0QEq-vnYohN",
        "outputId": "183d2ce2-65c2-49c9-ca8c-7f216083e4bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count     151254\n",
              "unique    151199\n",
              "top             \n",
              "freq          22\n",
              "Name: reviewText, dtype: object"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['reviewText'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "OLw7n7EcbEKV",
        "outputId": "ffd1376f-19a1-42f2-a991-e490d04d84d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    151254.000000\n",
              "mean        509.002142\n",
              "std         524.745639\n",
              "min           0.000000\n",
              "25%         191.000000\n",
              "50%         353.000000\n",
              "75%         644.000000\n",
              "max       29569.000000\n",
              "Name: reviewText, dtype: float64"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1728x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# review length\n",
        "rv_le=df['reviewText'].apply(lambda x:len(x) )\n",
        "rv_le.plot(kind='hist',bins=2000,figsize=(24, 8),xlabel=\"length of reviews\")\n",
        "rv_le.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTgNznNQ8aR1",
        "outputId": "3c1809ac-2fc3-4d67-ce83-7253c8687332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean of reviews per user: 10.302704175464887\n",
            "mean of words per review: 509.00214209211\n",
            "mean of words per user: 5244.098494652953\n"
          ]
        }
      ],
      "source": [
        "sum=0\n",
        "sum2=0\n",
        "for x in user_df['reviewText']:\n",
        "  sum+=len(x)\n",
        "  for y in x:\n",
        "   sum2 +=len(y)\n",
        "\n",
        "r_mean=sum/len(user_df['reviewText'])\n",
        "w_mean=sum2/(r_mean*len(user_df['reviewText']))\n",
        "print('mean of reviews per user:', r_mean)\n",
        "print('mean of words per review:', w_mean)\n",
        "print('mean of words per user:',r_mean * w_mean)\n",
        "# plot for reviews per user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    8713.000000\n",
              "mean       17.359578\n",
              "std        33.915980\n",
              "min         5.000000\n",
              "25%         6.000000\n",
              "50%         8.000000\n",
              "75%        14.000000\n",
              "max       742.000000\n",
              "Name: reviewText, dtype: float64"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1728x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "rv_le=item_df['reviewText'].apply(lambda x:len(x) )\n",
        "rv_le.plot(kind='hist',bins=2000,figsize=(24, 8),xlabel=\"length of reviews\")\n",
        "rv_le.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    14681.000000\n",
              "mean        10.302704\n",
              "std         10.581392\n",
              "min          5.000000\n",
              "25%          5.000000\n",
              "50%          7.000000\n",
              "75%         11.000000\n",
              "max        204.000000\n",
              "Name: reviewText, dtype: float64"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1728x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "rv_le=user_df['reviewText'].apply(lambda x:len(x) )\n",
        "rv_le.plot(kind='hist',bins=2000,figsize=(24, 8),xlabel=\"length of reviews\")\n",
        "rv_le.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nje_ix7gAaf3",
        "outputId": "c3260e88-aa4f-46f3-aa38-85cbb25e0219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean of reviews per item: 17.359577642603007\n",
            "mean of words per review: 509.00214209211\n",
            "mean of words per item: 8836.062205899232\n"
          ]
        }
      ],
      "source": [
        "sum=0\n",
        "sum2=0\n",
        "for x in item_df['reviewText']:\n",
        "  sum+=len(x)\n",
        "  for y in x:\n",
        "   sum2 +=len(y)\n",
        "\n",
        "r_mean=sum/len(item_df['reviewText'])\n",
        "w_mean=sum2/(r_mean*len(item_df['reviewText']))\n",
        "print('mean of reviews per item:', r_mean)\n",
        "print('mean of words per review:', w_mean)\n",
        "print('mean of words per item:',r_mean * w_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Data Splitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 15126  15127  15128 ... 151251 151252 151253] 136128\n",
            "[     0      1      2 ... 151251 151252 151253] 136128\n",
            "[     0      1      2 ... 151251 151252 151253] 136128\n",
            "[     0      1      2 ... 151251 151252 151253] 136128\n",
            "[     0      1      2 ... 151251 151252 151253] 136129\n",
            "[     0      1      2 ... 151251 151252 151253] 136129\n",
            "[     0      1      2 ... 151251 151252 151253] 136129\n",
            "[     0      1      2 ... 151251 151252 151253] 136129\n",
            "[     0      1      2 ... 151251 151252 151253] 136129\n",
            "[     0      1      2 ... 136126 136127 136128] 136129\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"train_df.csv\") & os.path.exists(\"test_df.csv\"):\n",
        "        train_df = pd.read_csv('train_df.csv')\n",
        "        test_df = pd.read_csv('test_df.csv')\n",
        "\n",
        "else:\n",
        "    current_fold=10\n",
        "    kfold = KFold(10)\n",
        "    random_iterator=kfold.split(df)\n",
        "    for i in range(current_fold):\n",
        "      train_index, test_index = next(random_iterator, None)\n",
        "      print(train_index,len(train_index))\n",
        "      train_df, test_df =df.iloc[train_index], df.iloc[test_index]\n",
        "    train_df.to_csv('train_df.csv')\n",
        "    test_df.to_csv('test_df.csv')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A1VEELTKS8NLZB</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>Just another flavor of Kit Kat but the taste i...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A14R9XMZVJ6INB</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>I bought this on impulse and it comes from Jap...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A27IQHDZFQFNGG</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>Really good. Great gift for any fan of green t...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A31QY5TASILE89</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>I had never had it before, was curious to see ...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A2LWK003FFMCI5</td>\n",
              "      <td>616719923X</td>\n",
              "      <td>I've been looking forward to trying these afte...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136124</th>\n",
              "      <td>A74CGCGJ11Y23</td>\n",
              "      <td>B009M516NE</td>\n",
              "      <td>I've made rice pilaf from scratch. I've also t...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136125</th>\n",
              "      <td>A36MP37DITBU6F</td>\n",
              "      <td>B009M516NE</td>\n",
              "      <td>This is a slightly mild flavored rice pilaf. t...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136126</th>\n",
              "      <td>A1JBBR4MNGQ70G</td>\n",
              "      <td>B009M516NE</td>\n",
              "      <td>I added a package of Albacore tuna to this mix...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136127</th>\n",
              "      <td>A2P739KOM4U5JB</td>\n",
              "      <td>B009M516NE</td>\n",
              "      <td>I am happy to report that Side Mates were grea...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136128</th>\n",
              "      <td>AT53ZTTO707MB</td>\n",
              "      <td>B009M516NE</td>\n",
              "      <td>This was a nice combo of rice, pasta and herbs...</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>136129 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                userID      itemID  \\\n",
              "0       A1VEELTKS8NLZB  616719923X   \n",
              "1       A14R9XMZVJ6INB  616719923X   \n",
              "2       A27IQHDZFQFNGG  616719923X   \n",
              "3       A31QY5TASILE89  616719923X   \n",
              "4       A2LWK003FFMCI5  616719923X   \n",
              "...                ...         ...   \n",
              "136124   A74CGCGJ11Y23  B009M516NE   \n",
              "136125  A36MP37DITBU6F  B009M516NE   \n",
              "136126  A1JBBR4MNGQ70G  B009M516NE   \n",
              "136127  A2P739KOM4U5JB  B009M516NE   \n",
              "136128   AT53ZTTO707MB  B009M516NE   \n",
              "\n",
              "                                               reviewText  rating  \n",
              "0       Just another flavor of Kit Kat but the taste i...     4.0  \n",
              "1       I bought this on impulse and it comes from Jap...     3.0  \n",
              "2       Really good. Great gift for any fan of green t...     4.0  \n",
              "3       I had never had it before, was curious to see ...     5.0  \n",
              "4       I've been looking forward to trying these afte...     4.0  \n",
              "...                                                   ...     ...  \n",
              "136124  I've made rice pilaf from scratch. I've also t...     4.0  \n",
              "136125  This is a slightly mild flavored rice pilaf. t...     4.0  \n",
              "136126  I added a package of Albacore tuna to this mix...     3.0  \n",
              "136127  I am happy to report that Side Mates were grea...     4.0  \n",
              "136128  This was a nice combo of rice, pasta and herbs...     3.0  \n",
              "\n",
              "[136129 rows x 4 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvMysaUvxxU3"
      },
      "source": [
        "# **Embedding Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WovD0fA5q-J6"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "embedding_dim=300\n",
        "min_frequent_word_num=5\n",
        "max_vocab_size=30000\n",
        "sequence_length=250\n",
        "document_length=40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  text = tf.strings.strip(text)\n",
        "  text= tf.strings.reduce_join( tf.strings.split(text)[:,:sequence_length-2],axis=-1,separator=' ')\n",
        "  return  text.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tf.device('/CPU:0'):\n",
        "    user_corpus =list(map(tf_lower_and_split_punct,user_df['reviewText'])) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_word2vec_model():\n",
        "    if os.path.exists(\"word2vec.wordvectors\"):\n",
        "        print(\"loaded from word2vec.wordvectors\")\n",
        "        return KeyedVectors.load(\"word2vec.wordvectors\",mmap='r')\n",
        "    else:\n",
        "        # downloading google news word2vec model\n",
        "        if not os.path.exists(\"word2vec_google.bin\"):\n",
        "            downloaded_model = api.load('word2vec-google-news-300')\n",
        "            downloaded_model.save_word2vec_format('word2vec_google.bin',binary=True)\n",
        "            del downloaded_model\n",
        "        # loading google news word2vec model\n",
        "        google_word2vec = KeyedVectors.load_word2vec_format(\"word2vec_google.bin\", binary=True)\n",
        "\n",
        "       # tokenizing the whole reviews in text_corpus\n",
        "        text_corpus=[]\n",
        "        for i,doc in enumerate(user_corpus):    # iterate through each sentence in the reviews\n",
        "            for rv in doc:\n",
        "                for sen in sent_tokenize(rv.decode(\"utf-8\")):\n",
        "                    temp = []\n",
        "                    # tokenize the sentence into words          \n",
        "                    for j in word_tokenize(sen):\n",
        "                        temp.append(j.lower())\n",
        "                    text_corpus.append(temp)\n",
        "                    del temp\n",
        "\n",
        "\n",
        "        # creating a new word2vec model and initializing it from pretrained google_word2vec\n",
        "        word2vec_model=Word2Vec( text_corpus,max_final_vocab=max_vocab_size,min_count=min_frequent_word_num ,vector_size= embedding_dim,window = 5,workers=16, sg=1,epochs=1)\n",
        "        word2vec_model.build_vocab(text_corpus)\n",
        "\n",
        "        word2vec_model.build_vocab([google_word2vec.index_to_key],update=True)\n",
        "        word2vec_model.wv.vectors_lockf = np.ones(len(word2vec_model.wv))\n",
        "        word2vec_model.wv.intersect_word2vec_format(\"word2vec_google.bin\",binary=True,lockf=1.0)\n",
        "        \n",
        "        # fine tuning the model and saving it\n",
        "        word2vec_model.train(text_corpus, epochs=5, total_examples=word2vec_model.corpus_count)\n",
        "        word2vec_model.wv.save(\"word2vec.wordvectors\")\n",
        "        \n",
        "        del google_word2vec\n",
        "        del text_corpus[:]\n",
        "        gc.collect()\n",
        "  \n",
        "        return word2vec_model.wv\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded from word2vec.wordvectors\n",
            "embedding matrix shape : ( 22284  , 300 )\n"
          ]
        }
      ],
      "source": [
        "# loading the embedding lookup matrix shape=( 30k ,300 ) approximately\n",
        "embedding_matrix = load_word2vec_model() \n",
        "print( \"embedding matrix shape : (\",len(embedding_matrix.index_to_key),\" ,\",embedding_matrix.vector_size,\")\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(22288, 300)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# adding 4 vectors for start end unknow and null tokens\n",
        "special_token_embedding = np.random.rand(4,embedding_matrix.vector_size)\n",
        "full_embedding_matrix = np.concatenate((special_token_embedding,embedding_matrix.vectors))\n",
        "full_embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example\n",
        "# embedding_matrix['keep']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVT3Ur54LsIz"
      },
      "source": [
        "# **Text Vectorization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of batches in train data 8508\n",
            "number of batches in test data 472\n"
          ]
        }
      ],
      "source": [
        "# hyperparameter\n",
        "\n",
        "batch_size = 16 # for encoder model\n",
        "batch_size_test = 32 # for encoder model\n",
        "num_batches = int(train_df.shape[0]/batch_size)\n",
        "num_batches_test = int(test_df.shape[0]/batch_size_test)\n",
        "print(\"number of batches in train data\",num_batches)\n",
        "print(\"number of batches in test data\",num_batches_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tf_lower_and_split_punct_enc(text):\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vectorization layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4QVJNF8EGGe"
      },
      "source": [
        "- The conversion of tokens to ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc_input_processor = tf.keras.layers.TextVectorization( max_tokens = max_vocab_size,\n",
        "                                                     standardize = tf_lower_and_split_punct_enc,\n",
        "                                                     output_sequence_length = sequence_length,  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc_input_processor.set_vocabulary(['','[UNK]','[START]','[END]'] +  embedding_matrix.index_to_key )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "333\n",
            "involved\n"
          ]
        }
      ],
      "source": [
        "print(enc_input_processor.get_vocabulary().index('involved'))\n",
        "print(enc_input_processor.get_vocabulary()[3351])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\"These candies are very good. They have a nice cherry flavor with just a little bit of sour thrown in. Most sour candies made these days make your eyes water and your mouth pucker. Not these gummi's, they have just enough of a sour taste to make them interesting but the sour taste is very very mild.My order came in great condition and very fresh.\", 'Decided to try these out since I loved the non sour versions of this gummy. Great in their own right but I have to say that the originals are the best. But these have a very unique taste. And not really that sour, just a little bitterness to them but nothing to take away from the over all taste.', 'received very fast and item  was in tact. If for personal use over a long period suggest divide it up and put into airtight storage bags or a sealer to keep soft and tasty!', \"These Haribo sour cherries are vibrantly colored and look like a red cherry on a green stem with a green leaf attached.  Each piece is over an inch long.  They are very cute!Unfortunately, they are sugary sweet and not sour to me at all.  They taste just like those cherry flavored cough drops that come in the red and white box and have no medicinal purposes whatsoever!If you like the flavor of artificial cherries, these are good.  They smell good too.  If you are looking for candies for a kids birthday party, these will definitely be a hit!  If you need a sugary pick-me-up, grab a few and you're set.  But if you really want something more sour, try another kind of Haribo candy.Added thought:  these would be beautiful cake decorations!\", 'I love Haribo products and have for nearly  years. It is nice to be able to buy them bulk on Amazon. These are delicious but since taste is subjective, try some for yourself.']\n",
            "tf.Tensor(\n",
            "[[   2   33  918 ...    0    0    0]\n",
            " [   2  485   10 ...    0    0    0]\n",
            " [   2  523   42 ...    0    0    0]\n",
            " [   2   33 2820 ...    0    0    0]\n",
            " [   2    7   61 ...    0    0    0]], shape=(5, 250), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# example\n",
        "print(item_df['reviewText'][1000])\n",
        "print(enc_input_processor(  item_df['reviewText'][1000]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Forming Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert string id to int id\n",
        "user_to_row = {}\n",
        "item_to_column = {}\n",
        "\n",
        "for i, user_id in enumerate(np.unique(df['userID'])):\n",
        "    user_to_row[user_id] = i\n",
        "\n",
        "for j, item_id in enumerate(np.unique(df['itemID'].tolist())):\n",
        "    item_to_column[item_id] = j"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "item_df[\"userID\"]=item_df[\"userID\"].apply(lambda x :[user_to_row[el] for el in x ])\n",
        "user_df[\"itemID\"]=user_df[\"itemID\"].apply(lambda x :[item_to_column[el] for el in x ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds_seq = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "       [user_to_row[dp] for dp in train_df['userID']],\n",
        "       [item_to_column[dp] for dp in train_df['itemID']]\n",
        "     \n",
        "    )\n",
        ").shuffle(131072).batch(batch_size,drop_remainder=True)\n",
        "\n",
        "test_ds_seq = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "       [user_to_row[dp] for dp in test_df['userID']],\n",
        "       [item_to_column[dp] for dp in test_df['itemID']]\n",
        "     \n",
        "    )\n",
        ").shuffle(16384).batch(batch_size,drop_remainder=True)\n",
        "\n",
        "\n",
        "train_ds_pmf = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "       [user_to_row[dp] for dp in train_df['userID']],\n",
        "       [item_to_column[dp] for dp in train_df['itemID']],\n",
        "       tf.cast(train_df['rating'],dtype=tf.float16)\n",
        "    )\n",
        ").shuffle(131072).batch(1024)\n",
        "test_ds_pmf = tf.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "      [user_to_row[dp] for dp in test_df['userID']],\n",
        "      [item_to_column[dp] for dp in test_df['itemID']],\n",
        "      tf.cast(test_df['rating'],dtype=tf.float16)\n",
        "    )\n",
        ").shuffle(16384).batch(256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train and test data number :  136129  ,  15125\n"
          ]
        }
      ],
      "source": [
        "train_data_num=train_df.shape[0]\n",
        "test_data_num=test_df.shape[0]\n",
        "print(\"train and test data number : \",train_data_num,\" , \",test_data_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV4PGxjttli6"
      },
      "source": [
        "# **User and Item Documents**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BwcnBKDxVMk"
      },
      "source": [
        "**User Documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_doc = list( map(enc_input_processor,[doc[:document_length] for doc in user_df['reviewText']]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWDCul-D_N99"
      },
      "source": [
        "**Item Documents**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "phoN10nsXZGc"
      },
      "outputs": [],
      "source": [
        "item_doc = list( map(enc_input_processor,[doc[:document_length] for doc in item_df['reviewText']]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# log files for training and test\n",
        "train_log_dir = 'logs/'  + '/train'\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
        "test_log_dir = 'logs/'  + '/test'\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
        "graph_log_dir = 'logs/graph/'  \n",
        "graph_summary_writer = tf.summary.create_file_writer(graph_log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RegularizationLoss(tf.keras.metrics.Metric):\n",
        "    def __init__(self,name='perplexity',**kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.regularizationloss=self.add_weight(name='rg',initializer='zeros')\n",
        "\n",
        "    def update_state(self, loss):\n",
        "        self.regularizationloss= loss\n",
        "\n",
        "    def result(self):\n",
        "        return self.regularizationloss\n",
        "\n",
        "\n",
        "class MSE(tf.keras.metrics.Metric):\n",
        "    def __init__(self,name='MSE',**kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.mse=self.add_weight(name='mse',initializer='zeros')\n",
        "\n",
        "    def update_state(self,mse):\n",
        "        self.mse =  mse\n",
        "\n",
        "    def result(self):\n",
        "        return self.mse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# metrics\n",
        "train_reg_loss = RegularizationLoss(name=\"reg\")\n",
        "train_mse = MSE(name='trian mse')\n",
        "test_mse = MSE(name='test mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Context-aware Matrix Factorization for Rating Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "#hyperparameters\n",
        "num_users=len(user_to_row)\n",
        "num_items=len(item_to_column)\n",
        "mean_inv = np.float32( train_df['rating'].mean())\n",
        "feature_num=128 # number of topics\n",
        "units=int(feature_num/2 )# gru units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uKWBtzhmMA5"
      },
      "source": [
        "## **PMF (Probabilistic Matrix Factorization)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "5JAoyyjOnJis"
      },
      "outputs": [],
      "source": [
        "class PMF():\n",
        "    def __init__(self, num_feat=16, epsilon=1, _lambda=0.2, momentum=0.8,  batch_size=1024,num_item=9000,num_user=15000,mean_inv=3.00000):\n",
        "        self.num_feat = num_feat  # Number of latent features,\n",
        "        self.epsilon = epsilon  # learning rate,\n",
        "        self._lambda = _lambda  # L2 regularization,\n",
        "        self.momentum = momentum  # momentum of the gradient,\n",
        "        self.batch_size = batch_size  # Number of training samples used in each batches (for SGD optimization)\n",
        "        self.num_item=num_item\n",
        "        self.num_user=num_user\n",
        "        self.V =  0.1 * np.random.randn(self.num_item, self.num_feat).astype(np.float64)  # Item feature vectors\n",
        "        self.U =  0.1 * np.random.randn(self.num_user, self.num_feat).astype(np.float64)  # User feature vectors\n",
        "        self.V_inc = np.zeros((self.num_item, self.num_feat),dtype=np.float64)\n",
        "        self.U_inc = np.zeros((self.num_user, self.num_feat),dtype=np.float64)\n",
        "        self.mean_inv= mean_inv  \n",
        "        \n",
        "        self.user_textual_features = np.zeros(shape=(num_users, feature_num),dtype=np.float32 )\n",
        "        self.item_textual_features =  np.zeros(shape=(num_items, feature_num),dtype=np.float32)\n",
        "        self.user_recommender_features =  np.zeros(shape=(num_users, feature_num),dtype=np.float64)\n",
        "        self.item_recommender_features =  np.zeros(shape=(num_items, feature_num),dtype=np.float64)\n",
        "    \n",
        "    def update_textual_features(self, user_textual_features , item_textual_features ):\n",
        "        self.user_textual_features = user_textual_features\n",
        "        self.item_textual_features = item_textual_features \n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "            \n",
        "            for batch_UserID,batch_ItemID, batch_rating  in train_ds_pmf:\n",
        "                  \n",
        "                # Compute Objective Function             \n",
        "                pred_out = np.sum(np.multiply(self.U[batch_UserID, :],\n",
        "                                              self.V[batch_ItemID, :]),\n",
        "                                  axis=1)  # mean_inv subtracted # np.multiply\n",
        "                \n",
        "                rawErr = pred_out - batch_rating.numpy() + self.mean_inv\n",
        "\n",
        "                # Compute gradients\n",
        "                Ix_User = 2 * np.multiply(rawErr[:, np.newaxis], self.V[batch_ItemID, :]) \\\n",
        "                       + self._lambda * (self.U[batch_UserID, :] - self.user_textual_features[batch_UserID,:] )\n",
        "                Ix_Item = 2 * np.multiply(rawErr[:, np.newaxis], self.U[batch_UserID, :]) \\\n",
        "                       + self._lambda * (self.V[batch_ItemID, :] - self.item_textual_features[batch_ItemID,:] ) \n",
        "                       # np.newaxis :increase the dimension\n",
        "               \n",
        "                dw_Item = np.zeros((self.num_item, self.num_feat))\n",
        "                dw_User = np.zeros((self.num_user, self.num_feat))\n",
        "                \n",
        "                # loop to aggreate the gradients of the same element\n",
        "                for i in range(batch_UserID.shape[0]):\n",
        "                    dw_Item[batch_ItemID[i], :] += Ix_Item[i, :]\n",
        "                    dw_User[batch_UserID[i], :] += Ix_User[i, :]\n",
        "\n",
        "                # Update with momentum\n",
        "              \n",
        "                self.V_inc = self.momentum * self.V_inc + self.epsilon * dw_Item / self.batch_size\n",
        "                self.U_inc = self.momentum * self.U_inc + self.epsilon * dw_User / self.batch_size\n",
        "                \n",
        "                self.V = self.V - self.V_inc\n",
        "                self.U = self.U - self.U_inc\n",
        "            \n",
        "                # Compute Objective Function after\n",
        "            self.evaluate()\n",
        "\n",
        "            return  tf.Variable(self.U) , tf.Variable(self.V)\n",
        "    \n",
        "    def evaluate(self):\n",
        "            rawErr= []\n",
        "            for batch_UserID,batch_ItemID, batch_rating  in train_ds_pmf:\n",
        "                        \n",
        "                pred_out = np.sum(np.multiply(self.U[batch_UserID, :],\n",
        "                                        self.V[batch_ItemID, :]),\n",
        "                                axis=1)  # mean_inv subtracted\n",
        "                rawErr.append(pred_out - batch_rating.numpy() + self.mean_inv)\n",
        "\n",
        "            obj = np.linalg.norm([item for sublist in rawErr for item in sublist]) ** 2 \n",
        "                   # + 0.5 * self._lambda * (np.linalg.norm(self.U - self.user_textual_features) ** 2 + np.linalg.norm(self.V - self.item_textual_features) ** 2)\n",
        "\n",
        "            train_mse.update_state(obj / train_data_num)\n",
        "           # Compute test error\n",
        "            rawErr= []\n",
        "            for batch_UserID,batch_ItemID, batch_rating  in test_ds_pmf:\n",
        "                pred_out = np.sum(np.multiply(self.U[batch_UserID, :],\n",
        "                                        self.V[batch_ItemID, :]),\n",
        "                                axis=1)  # mean_inv subtracted\n",
        "\n",
        "                rawErr.append(pred_out - batch_rating.numpy() + self.mean_inv)\n",
        "\n",
        "            test_mse.update_state(np.linalg.norm([item for sublist in rawErr for item in sublist])**2 / test_data_num)\n",
        "            # Print info\n",
        "            print('\\nTraining mse: %f, Test mse %f' % (train_mse.result(), test_mse.result()))\n",
        "\n",
        "\n",
        "    def predict(self, invID):\n",
        "        return np.dot(self.V, self.U[int(invID), :]) + self.mean_inv  \n",
        "\n",
        "    def topK(self, user_number, k=10):\n",
        "        inv_lst = np.random.choice(range(0,self.num_user),user_number).astype('int32')\n",
        "        pred = {}\n",
        "        for inv in inv_lst:\n",
        "            if pred.get(inv, None) is None:\n",
        "                pred[inv] = np.argsort(self.predict(inv))[-k:]  \n",
        "  \n",
        "        for _,user_id in enumerate(pred):\n",
        "           print(\"user id:\",user_id,\"recommended items\",pred[user_id])\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8UVYfYjX9YW"
      },
      "source": [
        "# **Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "TwRd0VNHkMf0"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,vocab_size, embedding_dim, enc_units):\n",
        "    super().__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding_dim=embedding_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding( self.vocab_size, self.embedding_dim, embeddings_initializer=keras.initializers.Constant(full_embedding_matrix),trainable=False)\n",
        "    self.gru= tf.keras.layers.Bidirectional(tf.keras.layers.GRU(  self.enc_units,return_state=True,  recurrent_initializer='glorot_uniform' ))\n",
        "\n",
        "\n",
        "  def call(self, reviews, state=None):\n",
        "    vectors = self.embedding(reviews)\n",
        "    _,encoder_forward_state,encoder_backward_state  = self.gru(vectors, initial_state=state)\n",
        " \n",
        "    return  tf.concat([ encoder_forward_state, encoder_backward_state],-1)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Topic Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Z3NlswFZRbub"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(tf.keras.Model): \n",
        "    def __init__(self,num_topic,num_item ,num_user, units ,embedding_dim,vocab_size ,sequence_length, num_batches,num_batches_test, batch_size, use_tf_function=False):\n",
        "        super().__init__()\n",
        "        self.num_batches_test = num_batches_test\n",
        "        self.num_batches = num_batches # train batch number\n",
        "        self.batch_size = batch_size\n",
        "        self.num_topic = num_topic\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.units = units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_item = num_item\n",
        "        self.num_user = num_user\n",
        "        self.alpha = 10.0\n",
        "        self.user_encoder = Encoder(vocab_size= self.vocab_size,  embedding_dim= self.embedding_dim, enc_units= self.units)\n",
        "        self.item_encoder = Encoder(vocab_size= self.vocab_size,  embedding_dim= self.embedding_dim, enc_units= self.units)     \n",
        "        self.optimizer_gn = tf.optimizers.Adam(1e-4)\n",
        "\n",
        "        self.user_textual_features = np.zeros(shape=(num_users, feature_num),dtype=np.float32 )\n",
        "        self.item_textual_features =  np.zeros(shape=(num_items, feature_num),dtype=np.float32)\n",
        "        self.user_recommender_features =  np.zeros(shape=(num_users, feature_num),dtype=np.float64)\n",
        "        self.item_recommender_features =  np.zeros(shape=(num_items, feature_num),dtype=np.float64)\n",
        "\n",
        "    \n",
        "    \n",
        "    def update_recommender_features(self,user_recommender_features , item_recommender_features):\n",
        "         self.user_recommender_features = user_recommender_features\n",
        "         self.item_recommender_features = item_recommender_features\n",
        "\n",
        "    def generate_textual_features(self):\n",
        "          for start_index in range(0, self.num_user, self.batch_size):\n",
        "            end_index = min(start_index + self.batch_size, self.num_user)                           \n",
        "            batch_userID = np.arange(start_index, end_index)\n",
        "\n",
        "            # fetch a batch of user doc\n",
        "            batch_userdoc_flattend=[]\n",
        "            userdoc_slice_idx=[0]\n",
        "            for doc_id in batch_userID:\n",
        "              userdoc_slice_idx.append(userdoc_slice_idx[-1] + user_doc[doc_id].shape[0])\n",
        "              batch_userdoc_flattend = np.append(batch_userdoc_flattend,user_doc[doc_id])\n",
        "            batch_userdoc = tf.reshape(tf.convert_to_tensor( batch_userdoc_flattend,dtype=tf.int32),[userdoc_slice_idx[-1],self.sequence_length])\n",
        "\n",
        "            user_enc_state = self.user_encoder(batch_userdoc) # user vector representations      \n",
        "            user_context_vector = tf.stack([ tf.reduce_mean(user_enc_state[ userdoc_slice_idx[sl_num] : userdoc_slice_idx[sl_num+1] ],0) for  sl_num in range(len(batch_userID))])\n",
        "            \n",
        "            self.user_textual_features[batch_userID]=user_context_vector.numpy()\n",
        "            \n",
        "          for start_index in range(0, self.num_item, self.batch_size):\n",
        "            end_index = min(start_index + self.batch_size, self.num_item)                           \n",
        "            batch_itemID = np.arange(start_index, end_index)\n",
        "\n",
        "            # fetch a batch of item doc\n",
        "            batch_itemdoc_flattend=[]\n",
        "            itemdoc_slice_idx=[0]\n",
        "            for doc_id in batch_itemID:\n",
        "              itemdoc_slice_idx.append(itemdoc_slice_idx[-1] + item_doc[doc_id].shape[0])\n",
        "              batch_itemdoc_flattend = np.append(batch_itemdoc_flattend,item_doc[doc_id])\n",
        "            batch_itemdoc = tf.reshape(tf.convert_to_tensor( batch_itemdoc_flattend,dtype=tf.int32),[itemdoc_slice_idx[-1],self.sequence_length])\n",
        "            \n",
        "            item_enc_state = self.item_encoder(batch_itemdoc) # item vector representations\n",
        "            item_context_vector = tf.stack([ tf.reduce_mean(item_enc_state[ itemdoc_slice_idx[sl_num] : itemdoc_slice_idx[sl_num+1] ],0) for  sl_num in range(len(batch_itemID))])\n",
        "            \n",
        "            self.item_textual_features[batch_itemID]=item_context_vector.numpy()\n",
        "           \n",
        "            return tf.Variable(self.user_textual_features) , tf.Variable(self.item_textual_features)\n",
        "\n",
        "    def train(self):\n",
        "          total_gen_loss=0\n",
        "          for t_step in range(self.num_batches):\n",
        " \n",
        "            (batch_userID,batch_itemID) = next(iter(train_ds_seq))\n",
        "            # fetch a batch of user doc\n",
        "            batch_userdoc_flattend=[]\n",
        "            userdoc_slice_idx=[0]\n",
        "            for doc_id in batch_userID:\n",
        "              userdoc_slice_idx.append(userdoc_slice_idx[-1] + user_doc[doc_id].shape[0])\n",
        "              batch_userdoc_flattend = np.append(batch_userdoc_flattend,user_doc[doc_id])\n",
        "            batch_userdoc = tf.reshape(tf.convert_to_tensor( batch_userdoc_flattend,dtype=tf.int32),[userdoc_slice_idx[-1],self.sequence_length])\n",
        "\n",
        "            # fetch a batch of item doc\n",
        "            batch_itemdoc_flattend=[]\n",
        "            itemdoc_slice_idx=[0]\n",
        "            for doc_id in batch_itemID:\n",
        "              itemdoc_slice_idx.append(itemdoc_slice_idx[-1] + item_doc[doc_id].shape[0])\n",
        "              batch_itemdoc_flattend = np.append(batch_itemdoc_flattend,item_doc[doc_id])\n",
        "            batch_itemdoc = tf.reshape(tf.convert_to_tensor( batch_itemdoc_flattend,dtype=tf.int32),[itemdoc_slice_idx[-1],self.sequence_length])\n",
        "            \n",
        "            with tf.GradientTape() as tape :              \n",
        "              user_enc_state = self.user_encoder(batch_userdoc) # user vector representations      \n",
        "                \n",
        "              user_context_vector = tf.stack([ tf.reduce_mean(user_enc_state[ userdoc_slice_idx[sl_num] : userdoc_slice_idx[sl_num+1] ],0) for  sl_num in range(self.batch_size)])\n",
        "              # regularization \n",
        "              user_regularization_loss = self.alpha * tf.norm(self.user_recommender_features[batch_userID.numpy()] - user_context_vector) **2\n",
        "            grad_enc = tape.gradient(user_regularization_loss,self.user_encoder.trainable_variables)\n",
        "            self.optimizer_gn.apply_gradients(zip( grad_enc, self.user_encoder.trainable_variables ))            \n",
        "            \n",
        "            total_gen_loss += user_regularization_loss\n",
        "            # total_regularization_loss += regularization_loss\n",
        "            if t_step % 10 == 0:\n",
        "              print(\"batch number: \",t_step,\"\\t encoder loss: \", user_regularization_loss)\n",
        "    \n",
        "          train_reg_loss.update_state(total_gen_loss / self.num_batches)\n",
        "\n",
        "    def test(self,num_steps):\n",
        "        total_loss=0\n",
        "        for step in range(num_steps):\n",
        "            #(batch_userID,batch_itemID ,_,_) = next(iter(test_ds_seq))\n",
        "            batch_userID = np.random.choice(range(0,self.num_user),self.batch_size).astype('int32')\n",
        "            batch_itemID = np.random.choice(range(0,self.num_item),self.batch_size).astype('int32') \n",
        "            context_vector = tf.concat([self.user_textual_features[batch_userID],self.item_textual_features[batch_itemID]],1)                    \n",
        "            predicted_samples,dec_prob = self.generate_sample(context_vector)\n",
        "            loss = self.loss_gn(predicted_samples ,tf.transpose( tf.stack(dec_prob), [1,0,2])  )\n",
        "            total_loss += loss\n",
        "            if step % 4 == 0:\n",
        "              print(\"batch number: \",step, \"\\tgen loss: \",loss.numpy())\n",
        "\n",
        "        test_plx_gen.update_state(total_loss / num_steps)\n",
        "  \n",
        "  \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Ploting Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-af8f02a507231cf0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-af8f02a507231cf0\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir logs --bind_all\n",
        "# deactive tracking protection of the page if you get 403 error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3YKdf1-uW8Q"
      },
      "source": [
        "# **Multi-Task Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "khZxlR9oRxVe"
      },
      "outputs": [],
      "source": [
        "class MultiTaskModel(tf.keras.Model):\n",
        "      def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pmf_model = PMF(num_feat=feature_num,num_item= num_items ,num_user= num_users, mean_inv=mean_inv)\n",
        "        self.seq2seq_model = Seq2Seq(num_topic=feature_num,num_item= num_items ,num_user= num_users,units=units, embedding_dim= embedding_dim,vocab_size=full_embedding_matrix.shape[0],sequence_length=sequence_length, num_batches= num_batches,num_batches_test=num_batches_test,batch_size= batch_size)      \n",
        "        self.user_textual_features = tf.Variable( np.zeros(shape=(num_users, feature_num),dtype=np.float32 ))\n",
        "        self.item_textual_features = tf.Variable( np.zeros(shape=(num_items, feature_num),dtype=np.float32))\n",
        "        self.user_recommender_features = tf.Variable( np.zeros(shape=(num_users, feature_num),dtype=np.float64))\n",
        "        self.item_recommender_features = tf.Variable( np.zeros(shape=(num_items, feature_num),dtype=np.float64))\n",
        "      \n",
        "      def predict(self,user_number=5):\n",
        "         prediction = self.pmf_model.topK(user_number)\n",
        "         self.generate_explanation(prediction)\n",
        "      \n",
        "      def generate_explanation(self,prediction):\n",
        "        for target_user_id in prediction:\n",
        "          for item_id in prediction[target_user_id]:\n",
        "            review_user_pair = item_df[[\"reviewText\",\"userID\"]].loc[item_id]\n",
        "            similar_user_idx = self.most_similar_users(target_user_id, review_user_pair[\"userID\"]) \n",
        "            relevant_reviews = [el[1]  for el in np.array(list( zip(review_user_pair[\"userID\"],review_user_pair[\"reviewText\"]))) if int(el[0]) in similar_user_idx]\n",
        "            positive_explanation = []\n",
        "            sentiment_scores = []\n",
        "            for rv in relevant_reviews:\n",
        "              sum_rv = summarize(rv, ratio=0.5)\n",
        "              sentences = sent_tokenize(sum_rv)\n",
        "              analyzer  = SentimentIntensityAnalyzer()\n",
        "              for sentence in sentences:\n",
        "                vs = analyzer.polarity_scores(sentence)\n",
        "                if vs[\"compound\"] > 0.1 :\n",
        "                   positive_explanation.append(sentence)\n",
        "                   sentiment_scores.append(float(vs[\"compound\"]))\n",
        " \n",
        "            exp_score_pair = list( zip(positive_explanation,sentiment_scores))\n",
        "            exp_score_pair.sort( reverse=True,key=lambda x: x[1])\n",
        "            print(\"\\n\\n\",\"user id :\",target_user_id,\"recommended item id :\",item_id,\"\\n explanation :\")\n",
        "            for el in exp_score_pair[:4]:\n",
        "              print(\" - \",el[0])\n",
        "\n",
        "            \n",
        "      def most_similar_users(self,target_user_id, user_idx,top_k=5):\n",
        "         cosine_loss = tf.keras.losses.CosineSimilarity(axis=1,  reduction=tf.keras.losses.Reduction.NONE)\n",
        "         a=tf.tile(tf.expand_dims( self.user_textual_features[target_user_id],0),[len(user_idx),1])\n",
        "         b=tf.gather(self.user_textual_features , user_idx)\n",
        "         cosine_similarity = cosine_loss( a, b ).numpy()\n",
        "         most_similar_idx =  cosine_similarity.argsort()[-top_k:][::-1]\n",
        "         return np.array(user_idx)[most_similar_idx]\n",
        "\n",
        "\n",
        "         \n",
        "      def train(self,n_epochs):    \n",
        "          ckpt.restore(manager.latest_checkpoint)\n",
        "          if manager.latest_checkpoint:\n",
        "            print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
        "          else:\n",
        "            print(\"Initializing from scratch.\")\n",
        "          #tf.profiler.experimental.start('logs')\n",
        "          for epoch in range(n_epochs):\n",
        "              if epoch == 0:\n",
        "                 self.pmf_model.U = self.user_recommender_features.numpy()\n",
        "                 self.pmf_model.V = self.item_recommender_features.numpy()\n",
        "              \n",
        "              print(\"\\n\\nepoch : \", int(ckpt.step))\n",
        "              self.user_textual_features,self.item_textual_features = self.seq2seq_model.generate_textual_features()\n",
        "              self.pmf_model.update_textual_features( self.user_textual_features.numpy(),self.item_textual_features.numpy() )\n",
        "              \n",
        "              print(\"********************************************* PMF Model Training Turn *********************************************\")\n",
        "              self.user_recommender_features , self.item_recommender_features = self.pmf_model.train()\n",
        "              print(\"\\n\\n******************************************* Seq2Seq Model Training Turn *******************************************\")                                         \n",
        "              self.seq2seq_model.update_recommender_features(self.user_recommender_features.numpy() , self.item_recommender_features.numpy())\n",
        "              self.seq2seq_model.train()\n",
        "\n",
        "              with train_summary_writer.as_default():\n",
        "                  tf.summary.scalar('perplexity for Seq2Seq model', train_reg_loss.result(), step=int(ckpt.step))\n",
        "                  tf.summary.scalar('MSE for PMF model',train_mse.result(),step=int(ckpt.step))\n",
        "  \n",
        "              with test_summary_writer.as_default():\n",
        "                  tf.summary.scalar('MSE for PMF model',test_mse.result(),step=int(ckpt.step))\n",
        "\n",
        "              ckpt.step.assign_add(1)\n",
        "              if int(ckpt.step) % 2 ==0:\n",
        "                save_path = manager.save()\n",
        "                print(\"Saved checkpoint for epoch {}: {}\".format(int(ckpt.step), save_path))\n",
        "            \n",
        "          #tf.profiler.experimental.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrzxSK6IUr6O",
        "outputId": "212b1776-5de7-4896-f09a-4ea54c2405df"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "gc.collect(0)\n",
        "gc.collect(1)\n",
        "gc.collect(2)\n",
        "\n",
        "# creating an instance of Multi_Task Model\n",
        "mt_model = MultiTaskModel()\n",
        "ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=tf.keras.optimizers.Adam(), net=mt_model)\n",
        "manager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=200)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing from scratch.\n",
            "\n",
            "\n",
            "epoch :  0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-12-25 12:25:25.673617: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8500\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "********************************************* PMF Model Training Turn *********************************************\n",
            "\n",
            "Training mse: 1.189532, Test mse 1.167727\n",
            "\n",
            "\n",
            "******************************************* Seq2Seq Model Training Turn *******************************************\n",
            "batch number:  0 \t encoder loss:  tf.Tensor(2988.989, shape=(), dtype=float32)\n",
            "batch number:  10 \t encoder loss:  tf.Tensor(1924.7969, shape=(), dtype=float32)\n",
            "batch number:  20 \t encoder loss:  tf.Tensor(1155.266, shape=(), dtype=float32)\n",
            "batch number:  30 \t encoder loss:  tf.Tensor(668.23114, shape=(), dtype=float32)\n",
            "batch number:  40 \t encoder loss:  tf.Tensor(341.96152, shape=(), dtype=float32)\n",
            "batch number:  50 \t encoder loss:  tf.Tensor(191.66933, shape=(), dtype=float32)\n",
            "batch number:  60 \t encoder loss:  tf.Tensor(96.30415, shape=(), dtype=float32)\n",
            "batch number:  70 \t encoder loss:  tf.Tensor(49.368355, shape=(), dtype=float32)\n",
            "batch number:  80 \t encoder loss:  tf.Tensor(27.769424, shape=(), dtype=float32)\n",
            "batch number:  90 \t encoder loss:  tf.Tensor(17.68387, shape=(), dtype=float32)\n",
            "batch number:  100 \t encoder loss:  tf.Tensor(14.906866, shape=(), dtype=float32)\n",
            "batch number:  110 \t encoder loss:  tf.Tensor(16.997051, shape=(), dtype=float32)\n",
            "batch number:  120 \t encoder loss:  tf.Tensor(9.523326, shape=(), dtype=float32)\n",
            "batch number:  130 \t encoder loss:  tf.Tensor(10.385904, shape=(), dtype=float32)\n",
            "batch number:  140 \t encoder loss:  tf.Tensor(10.589403, shape=(), dtype=float32)\n",
            "batch number:  150 \t encoder loss:  tf.Tensor(12.206076, shape=(), dtype=float32)\n",
            "batch number:  160 \t encoder loss:  tf.Tensor(12.959644, shape=(), dtype=float32)\n",
            "batch number:  170 \t encoder loss:  tf.Tensor(7.2421336, shape=(), dtype=float32)\n",
            "batch number:  180 \t encoder loss:  tf.Tensor(12.487845, shape=(), dtype=float32)\n",
            "batch number:  190 \t encoder loss:  tf.Tensor(10.570463, shape=(), dtype=float32)\n",
            "batch number:  200 \t encoder loss:  tf.Tensor(11.9817915, shape=(), dtype=float32)\n",
            "batch number:  210 \t encoder loss:  tf.Tensor(23.836927, shape=(), dtype=float32)\n",
            "batch number:  220 \t encoder loss:  tf.Tensor(5.641391, shape=(), dtype=float32)\n",
            "batch number:  230 \t encoder loss:  tf.Tensor(6.745719, shape=(), dtype=float32)\n",
            "batch number:  240 \t encoder loss:  tf.Tensor(7.9192934, shape=(), dtype=float32)\n",
            "batch number:  250 \t encoder loss:  tf.Tensor(9.749828, shape=(), dtype=float32)\n",
            "batch number:  260 \t encoder loss:  tf.Tensor(8.157285, shape=(), dtype=float32)\n",
            "batch number:  270 \t encoder loss:  tf.Tensor(9.252385, shape=(), dtype=float32)\n",
            "batch number:  280 \t encoder loss:  tf.Tensor(5.4264536, shape=(), dtype=float32)\n",
            "batch number:  290 \t encoder loss:  tf.Tensor(8.772023, shape=(), dtype=float32)\n",
            "batch number:  300 \t encoder loss:  tf.Tensor(6.6572976, shape=(), dtype=float32)\n",
            "batch number:  310 \t encoder loss:  tf.Tensor(6.733756, shape=(), dtype=float32)\n",
            "batch number:  320 \t encoder loss:  tf.Tensor(10.256377, shape=(), dtype=float32)\n",
            "batch number:  330 \t encoder loss:  tf.Tensor(6.702319, shape=(), dtype=float32)\n",
            "batch number:  340 \t encoder loss:  tf.Tensor(4.5071545, shape=(), dtype=float32)\n",
            "batch number:  350 \t encoder loss:  tf.Tensor(4.5802326, shape=(), dtype=float32)\n",
            "batch number:  360 \t encoder loss:  tf.Tensor(6.0523577, shape=(), dtype=float32)\n",
            "batch number:  370 \t encoder loss:  tf.Tensor(5.589461, shape=(), dtype=float32)\n",
            "batch number:  380 \t encoder loss:  tf.Tensor(4.282018, shape=(), dtype=float32)\n",
            "batch number:  390 \t encoder loss:  tf.Tensor(6.130543, shape=(), dtype=float32)\n",
            "batch number:  400 \t encoder loss:  tf.Tensor(5.630887, shape=(), dtype=float32)\n",
            "batch number:  410 \t encoder loss:  tf.Tensor(8.328413, shape=(), dtype=float32)\n",
            "batch number:  420 \t encoder loss:  tf.Tensor(5.795122, shape=(), dtype=float32)\n",
            "batch number:  430 \t encoder loss:  tf.Tensor(4.9818664, shape=(), dtype=float32)\n",
            "batch number:  440 \t encoder loss:  tf.Tensor(8.011731, shape=(), dtype=float32)\n",
            "batch number:  450 \t encoder loss:  tf.Tensor(8.8659725, shape=(), dtype=float32)\n",
            "batch number:  460 \t encoder loss:  tf.Tensor(6.7187796, shape=(), dtype=float32)\n",
            "batch number:  470 \t encoder loss:  tf.Tensor(7.0496264, shape=(), dtype=float32)\n",
            "batch number:  480 \t encoder loss:  tf.Tensor(5.986356, shape=(), dtype=float32)\n",
            "batch number:  490 \t encoder loss:  tf.Tensor(5.2811933, shape=(), dtype=float32)\n",
            "batch number:  500 \t encoder loss:  tf.Tensor(5.7813096, shape=(), dtype=float32)\n",
            "batch number:  510 \t encoder loss:  tf.Tensor(5.013384, shape=(), dtype=float32)\n",
            "batch number:  520 \t encoder loss:  tf.Tensor(5.693567, shape=(), dtype=float32)\n",
            "batch number:  530 \t encoder loss:  tf.Tensor(4.920094, shape=(), dtype=float32)\n",
            "batch number:  540 \t encoder loss:  tf.Tensor(3.2094927, shape=(), dtype=float32)\n",
            "batch number:  550 \t encoder loss:  tf.Tensor(4.642076, shape=(), dtype=float32)\n",
            "batch number:  560 \t encoder loss:  tf.Tensor(6.504829, shape=(), dtype=float32)\n",
            "batch number:  570 \t encoder loss:  tf.Tensor(4.473092, shape=(), dtype=float32)\n",
            "batch number:  580 \t encoder loss:  tf.Tensor(7.125283, shape=(), dtype=float32)\n",
            "batch number:  590 \t encoder loss:  tf.Tensor(3.4191518, shape=(), dtype=float32)\n",
            "batch number:  600 \t encoder loss:  tf.Tensor(4.672809, shape=(), dtype=float32)\n",
            "batch number:  610 \t encoder loss:  tf.Tensor(4.950573, shape=(), dtype=float32)\n",
            "batch number:  620 \t encoder loss:  tf.Tensor(5.3367176, shape=(), dtype=float32)\n",
            "batch number:  630 \t encoder loss:  tf.Tensor(4.250311, shape=(), dtype=float32)\n",
            "batch number:  640 \t encoder loss:  tf.Tensor(3.364452, shape=(), dtype=float32)\n",
            "batch number:  650 \t encoder loss:  tf.Tensor(2.7334352, shape=(), dtype=float32)\n",
            "batch number:  660 \t encoder loss:  tf.Tensor(5.1454306, shape=(), dtype=float32)\n",
            "batch number:  670 \t encoder loss:  tf.Tensor(5.7492523, shape=(), dtype=float32)\n",
            "batch number:  680 \t encoder loss:  tf.Tensor(4.639089, shape=(), dtype=float32)\n",
            "batch number:  690 \t encoder loss:  tf.Tensor(4.59488, shape=(), dtype=float32)\n",
            "batch number:  700 \t encoder loss:  tf.Tensor(3.1326718, shape=(), dtype=float32)\n",
            "batch number:  710 \t encoder loss:  tf.Tensor(4.087263, shape=(), dtype=float32)\n",
            "batch number:  720 \t encoder loss:  tf.Tensor(2.9684367, shape=(), dtype=float32)\n",
            "batch number:  730 \t encoder loss:  tf.Tensor(2.4513772, shape=(), dtype=float32)\n",
            "batch number:  740 \t encoder loss:  tf.Tensor(2.7118087, shape=(), dtype=float32)\n",
            "batch number:  750 \t encoder loss:  tf.Tensor(9.153143, shape=(), dtype=float32)\n",
            "batch number:  760 \t encoder loss:  tf.Tensor(4.492032, shape=(), dtype=float32)\n",
            "batch number:  770 \t encoder loss:  tf.Tensor(2.4156847, shape=(), dtype=float32)\n",
            "batch number:  780 \t encoder loss:  tf.Tensor(3.293603, shape=(), dtype=float32)\n",
            "batch number:  790 \t encoder loss:  tf.Tensor(4.5624785, shape=(), dtype=float32)\n",
            "batch number:  800 \t encoder loss:  tf.Tensor(4.760101, shape=(), dtype=float32)\n",
            "batch number:  810 \t encoder loss:  tf.Tensor(2.480457, shape=(), dtype=float32)\n",
            "batch number:  820 \t encoder loss:  tf.Tensor(2.6905951, shape=(), dtype=float32)\n",
            "batch number:  830 \t encoder loss:  tf.Tensor(3.002512, shape=(), dtype=float32)\n",
            "batch number:  840 \t encoder loss:  tf.Tensor(3.461668, shape=(), dtype=float32)\n",
            "batch number:  850 \t encoder loss:  tf.Tensor(2.494532, shape=(), dtype=float32)\n",
            "batch number:  860 \t encoder loss:  tf.Tensor(3.2428188, shape=(), dtype=float32)\n",
            "batch number:  870 \t encoder loss:  tf.Tensor(4.1023726, shape=(), dtype=float32)\n",
            "batch number:  880 \t encoder loss:  tf.Tensor(3.0483868, shape=(), dtype=float32)\n",
            "batch number:  890 \t encoder loss:  tf.Tensor(4.782472, shape=(), dtype=float32)\n",
            "batch number:  900 \t encoder loss:  tf.Tensor(6.401326, shape=(), dtype=float32)\n",
            "batch number:  910 \t encoder loss:  tf.Tensor(4.0442996, shape=(), dtype=float32)\n",
            "batch number:  920 \t encoder loss:  tf.Tensor(3.8502264, shape=(), dtype=float32)\n",
            "batch number:  930 \t encoder loss:  tf.Tensor(2.643554, shape=(), dtype=float32)\n",
            "batch number:  940 \t encoder loss:  tf.Tensor(3.324737, shape=(), dtype=float32)\n",
            "batch number:  950 \t encoder loss:  tf.Tensor(2.357531, shape=(), dtype=float32)\n",
            "batch number:  960 \t encoder loss:  tf.Tensor(2.1438222, shape=(), dtype=float32)\n",
            "batch number:  970 \t encoder loss:  tf.Tensor(2.756684, shape=(), dtype=float32)\n",
            "batch number:  980 \t encoder loss:  tf.Tensor(2.260505, shape=(), dtype=float32)\n",
            "batch number:  990 \t encoder loss:  tf.Tensor(3.0153966, shape=(), dtype=float32)\n",
            "batch number:  1000 \t encoder loss:  tf.Tensor(3.590976, shape=(), dtype=float32)\n",
            "batch number:  1010 \t encoder loss:  tf.Tensor(2.1563416, shape=(), dtype=float32)\n",
            "batch number:  1020 \t encoder loss:  tf.Tensor(3.1666875, shape=(), dtype=float32)\n",
            "batch number:  1030 \t encoder loss:  tf.Tensor(2.8495457, shape=(), dtype=float32)\n",
            "batch number:  1040 \t encoder loss:  tf.Tensor(2.3258839, shape=(), dtype=float32)\n",
            "batch number:  1050 \t encoder loss:  tf.Tensor(4.361581, shape=(), dtype=float32)\n",
            "batch number:  1060 \t encoder loss:  tf.Tensor(1.9971198, shape=(), dtype=float32)\n",
            "batch number:  1070 \t encoder loss:  tf.Tensor(4.843528, shape=(), dtype=float32)\n",
            "batch number:  1080 \t encoder loss:  tf.Tensor(1.836314, shape=(), dtype=float32)\n",
            "batch number:  1090 \t encoder loss:  tf.Tensor(3.1241653, shape=(), dtype=float32)\n",
            "batch number:  1100 \t encoder loss:  tf.Tensor(3.9445612, shape=(), dtype=float32)\n",
            "batch number:  1110 \t encoder loss:  tf.Tensor(3.5349166, shape=(), dtype=float32)\n",
            "batch number:  1120 \t encoder loss:  tf.Tensor(2.1759477, shape=(), dtype=float32)\n",
            "batch number:  1130 \t encoder loss:  tf.Tensor(2.9281263, shape=(), dtype=float32)\n",
            "batch number:  1140 \t encoder loss:  tf.Tensor(1.6951909, shape=(), dtype=float32)\n",
            "batch number:  1150 \t encoder loss:  tf.Tensor(4.078078, shape=(), dtype=float32)\n",
            "batch number:  1160 \t encoder loss:  tf.Tensor(2.993413, shape=(), dtype=float32)\n",
            "batch number:  1170 \t encoder loss:  tf.Tensor(2.9625368, shape=(), dtype=float32)\n",
            "batch number:  1180 \t encoder loss:  tf.Tensor(2.1326926, shape=(), dtype=float32)\n",
            "batch number:  1190 \t encoder loss:  tf.Tensor(1.9450204, shape=(), dtype=float32)\n",
            "batch number:  1200 \t encoder loss:  tf.Tensor(1.7001121, shape=(), dtype=float32)\n",
            "batch number:  1210 \t encoder loss:  tf.Tensor(2.8445807, shape=(), dtype=float32)\n",
            "batch number:  1220 \t encoder loss:  tf.Tensor(2.2012947, shape=(), dtype=float32)\n",
            "batch number:  1230 \t encoder loss:  tf.Tensor(1.6312808, shape=(), dtype=float32)\n",
            "batch number:  1240 \t encoder loss:  tf.Tensor(2.8859246, shape=(), dtype=float32)\n",
            "batch number:  1250 \t encoder loss:  tf.Tensor(1.736554, shape=(), dtype=float32)\n",
            "batch number:  1260 \t encoder loss:  tf.Tensor(5.432686, shape=(), dtype=float32)\n",
            "batch number:  1270 \t encoder loss:  tf.Tensor(2.7615705, shape=(), dtype=float32)\n",
            "batch number:  1280 \t encoder loss:  tf.Tensor(1.968168, shape=(), dtype=float32)\n",
            "batch number:  1290 \t encoder loss:  tf.Tensor(1.9503171, shape=(), dtype=float32)\n",
            "batch number:  1300 \t encoder loss:  tf.Tensor(6.676098, shape=(), dtype=float32)\n",
            "batch number:  1310 \t encoder loss:  tf.Tensor(3.2425137, shape=(), dtype=float32)\n",
            "batch number:  1320 \t encoder loss:  tf.Tensor(3.533978, shape=(), dtype=float32)\n",
            "batch number:  1330 \t encoder loss:  tf.Tensor(2.0095067, shape=(), dtype=float32)\n",
            "batch number:  1340 \t encoder loss:  tf.Tensor(5.6163654, shape=(), dtype=float32)\n",
            "batch number:  1350 \t encoder loss:  tf.Tensor(1.5633594, shape=(), dtype=float32)\n",
            "batch number:  1360 \t encoder loss:  tf.Tensor(1.9476852, shape=(), dtype=float32)\n",
            "batch number:  1370 \t encoder loss:  tf.Tensor(2.7527924, shape=(), dtype=float32)\n",
            "batch number:  1380 \t encoder loss:  tf.Tensor(2.0860739, shape=(), dtype=float32)\n",
            "batch number:  1390 \t encoder loss:  tf.Tensor(2.2640953, shape=(), dtype=float32)\n",
            "batch number:  1400 \t encoder loss:  tf.Tensor(5.738704, shape=(), dtype=float32)\n",
            "batch number:  1410 \t encoder loss:  tf.Tensor(3.0472972, shape=(), dtype=float32)\n",
            "batch number:  1420 \t encoder loss:  tf.Tensor(3.115899, shape=(), dtype=float32)\n",
            "batch number:  1430 \t encoder loss:  tf.Tensor(1.5098829, shape=(), dtype=float32)\n",
            "batch number:  1440 \t encoder loss:  tf.Tensor(3.6759284, shape=(), dtype=float32)\n",
            "batch number:  1450 \t encoder loss:  tf.Tensor(1.8308988, shape=(), dtype=float32)\n",
            "batch number:  1460 \t encoder loss:  tf.Tensor(2.084001, shape=(), dtype=float32)\n",
            "batch number:  1470 \t encoder loss:  tf.Tensor(3.6181083, shape=(), dtype=float32)\n",
            "batch number:  1480 \t encoder loss:  tf.Tensor(6.3228245, shape=(), dtype=float32)\n",
            "batch number:  1490 \t encoder loss:  tf.Tensor(1.874722, shape=(), dtype=float32)\n",
            "batch number:  1500 \t encoder loss:  tf.Tensor(5.158754, shape=(), dtype=float32)\n",
            "batch number:  1510 \t encoder loss:  tf.Tensor(6.1657753, shape=(), dtype=float32)\n",
            "batch number:  1520 \t encoder loss:  tf.Tensor(1.8056, shape=(), dtype=float32)\n",
            "batch number:  1530 \t encoder loss:  tf.Tensor(1.5346701, shape=(), dtype=float32)\n",
            "batch number:  1540 \t encoder loss:  tf.Tensor(2.2179723, shape=(), dtype=float32)\n",
            "batch number:  1550 \t encoder loss:  tf.Tensor(4.145475, shape=(), dtype=float32)\n",
            "batch number:  1560 \t encoder loss:  tf.Tensor(2.0239003, shape=(), dtype=float32)\n",
            "batch number:  1570 \t encoder loss:  tf.Tensor(1.8777293, shape=(), dtype=float32)\n",
            "batch number:  1580 \t encoder loss:  tf.Tensor(2.6197762, shape=(), dtype=float32)\n",
            "batch number:  1590 \t encoder loss:  tf.Tensor(2.041943, shape=(), dtype=float32)\n",
            "batch number:  1600 \t encoder loss:  tf.Tensor(1.9203954, shape=(), dtype=float32)\n",
            "batch number:  1610 \t encoder loss:  tf.Tensor(1.9520223, shape=(), dtype=float32)\n",
            "batch number:  1620 \t encoder loss:  tf.Tensor(4.9916925, shape=(), dtype=float32)\n",
            "batch number:  1630 \t encoder loss:  tf.Tensor(2.325613, shape=(), dtype=float32)\n",
            "batch number:  1640 \t encoder loss:  tf.Tensor(1.6032186, shape=(), dtype=float32)\n",
            "batch number:  1650 \t encoder loss:  tf.Tensor(1.9187467, shape=(), dtype=float32)\n",
            "batch number:  1660 \t encoder loss:  tf.Tensor(2.8339849, shape=(), dtype=float32)\n",
            "batch number:  1670 \t encoder loss:  tf.Tensor(2.8372927, shape=(), dtype=float32)\n",
            "batch number:  1680 \t encoder loss:  tf.Tensor(3.2978468, shape=(), dtype=float32)\n",
            "batch number:  1690 \t encoder loss:  tf.Tensor(3.9156117, shape=(), dtype=float32)\n",
            "batch number:  1700 \t encoder loss:  tf.Tensor(2.248966, shape=(), dtype=float32)\n",
            "batch number:  1710 \t encoder loss:  tf.Tensor(1.3872014, shape=(), dtype=float32)\n",
            "batch number:  1720 \t encoder loss:  tf.Tensor(1.9447956, shape=(), dtype=float32)\n",
            "batch number:  1730 \t encoder loss:  tf.Tensor(1.9744719, shape=(), dtype=float32)\n",
            "batch number:  1740 \t encoder loss:  tf.Tensor(2.6545906, shape=(), dtype=float32)\n",
            "batch number:  1750 \t encoder loss:  tf.Tensor(1.7958226, shape=(), dtype=float32)\n",
            "batch number:  1760 \t encoder loss:  tf.Tensor(1.5116308, shape=(), dtype=float32)\n",
            "batch number:  1770 \t encoder loss:  tf.Tensor(1.6378205, shape=(), dtype=float32)\n",
            "batch number:  1780 \t encoder loss:  tf.Tensor(1.6985109, shape=(), dtype=float32)\n",
            "batch number:  1790 \t encoder loss:  tf.Tensor(2.3284514, shape=(), dtype=float32)\n",
            "batch number:  1800 \t encoder loss:  tf.Tensor(1.1205546, shape=(), dtype=float32)\n",
            "batch number:  1810 \t encoder loss:  tf.Tensor(2.820354, shape=(), dtype=float32)\n",
            "batch number:  1820 \t encoder loss:  tf.Tensor(2.0156116, shape=(), dtype=float32)\n",
            "batch number:  1830 \t encoder loss:  tf.Tensor(3.510156, shape=(), dtype=float32)\n",
            "batch number:  1840 \t encoder loss:  tf.Tensor(2.415059, shape=(), dtype=float32)\n",
            "batch number:  1850 \t encoder loss:  tf.Tensor(2.3756094, shape=(), dtype=float32)\n",
            "batch number:  1860 \t encoder loss:  tf.Tensor(2.1037605, shape=(), dtype=float32)\n",
            "batch number:  1870 \t encoder loss:  tf.Tensor(5.986152, shape=(), dtype=float32)\n",
            "batch number:  1880 \t encoder loss:  tf.Tensor(4.9839044, shape=(), dtype=float32)\n",
            "batch number:  1890 \t encoder loss:  tf.Tensor(1.2542715, shape=(), dtype=float32)\n",
            "batch number:  1900 \t encoder loss:  tf.Tensor(2.30041, shape=(), dtype=float32)\n",
            "batch number:  1910 \t encoder loss:  tf.Tensor(1.3998241, shape=(), dtype=float32)\n",
            "batch number:  1920 \t encoder loss:  tf.Tensor(6.324513, shape=(), dtype=float32)\n",
            "batch number:  1930 \t encoder loss:  tf.Tensor(1.4448696, shape=(), dtype=float32)\n",
            "batch number:  1940 \t encoder loss:  tf.Tensor(1.7298493, shape=(), dtype=float32)\n",
            "batch number:  1950 \t encoder loss:  tf.Tensor(3.205565, shape=(), dtype=float32)\n",
            "batch number:  1960 \t encoder loss:  tf.Tensor(3.1146936, shape=(), dtype=float32)\n",
            "batch number:  1970 \t encoder loss:  tf.Tensor(1.3913642, shape=(), dtype=float32)\n",
            "batch number:  1980 \t encoder loss:  tf.Tensor(1.7352138, shape=(), dtype=float32)\n",
            "batch number:  1990 \t encoder loss:  tf.Tensor(2.4143302, shape=(), dtype=float32)\n",
            "batch number:  2000 \t encoder loss:  tf.Tensor(1.2538586, shape=(), dtype=float32)\n",
            "batch number:  2010 \t encoder loss:  tf.Tensor(2.6475775, shape=(), dtype=float32)\n",
            "batch number:  2020 \t encoder loss:  tf.Tensor(1.9394021, shape=(), dtype=float32)\n",
            "batch number:  2030 \t encoder loss:  tf.Tensor(1.4096798, shape=(), dtype=float32)\n",
            "batch number:  2040 \t encoder loss:  tf.Tensor(1.0857086, shape=(), dtype=float32)\n",
            "batch number:  2050 \t encoder loss:  tf.Tensor(4.7102814, shape=(), dtype=float32)\n",
            "batch number:  2060 \t encoder loss:  tf.Tensor(1.6510696, shape=(), dtype=float32)\n",
            "batch number:  2070 \t encoder loss:  tf.Tensor(3.139701, shape=(), dtype=float32)\n",
            "batch number:  2080 \t encoder loss:  tf.Tensor(1.2293098, shape=(), dtype=float32)\n",
            "batch number:  2090 \t encoder loss:  tf.Tensor(3.416232, shape=(), dtype=float32)\n",
            "batch number:  2100 \t encoder loss:  tf.Tensor(1.6357523, shape=(), dtype=float32)\n",
            "batch number:  2110 \t encoder loss:  tf.Tensor(1.9190431, shape=(), dtype=float32)\n",
            "batch number:  2120 \t encoder loss:  tf.Tensor(2.413709, shape=(), dtype=float32)\n",
            "batch number:  2130 \t encoder loss:  tf.Tensor(3.639073, shape=(), dtype=float32)\n",
            "batch number:  2140 \t encoder loss:  tf.Tensor(1.5795599, shape=(), dtype=float32)\n",
            "batch number:  2150 \t encoder loss:  tf.Tensor(3.461987, shape=(), dtype=float32)\n",
            "batch number:  2160 \t encoder loss:  tf.Tensor(1.2187756, shape=(), dtype=float32)\n",
            "batch number:  2170 \t encoder loss:  tf.Tensor(1.789421, shape=(), dtype=float32)\n",
            "batch number:  2180 \t encoder loss:  tf.Tensor(1.4846375, shape=(), dtype=float32)\n",
            "batch number:  2190 \t encoder loss:  tf.Tensor(1.500248, shape=(), dtype=float32)\n",
            "batch number:  2200 \t encoder loss:  tf.Tensor(1.5314431, shape=(), dtype=float32)\n",
            "batch number:  2210 \t encoder loss:  tf.Tensor(1.4515543, shape=(), dtype=float32)\n",
            "batch number:  2220 \t encoder loss:  tf.Tensor(1.7416428, shape=(), dtype=float32)\n",
            "batch number:  2230 \t encoder loss:  tf.Tensor(1.9786257, shape=(), dtype=float32)\n",
            "batch number:  2240 \t encoder loss:  tf.Tensor(1.353183, shape=(), dtype=float32)\n",
            "batch number:  2250 \t encoder loss:  tf.Tensor(1.6958041, shape=(), dtype=float32)\n",
            "batch number:  2260 \t encoder loss:  tf.Tensor(1.6856276, shape=(), dtype=float32)\n",
            "batch number:  2270 \t encoder loss:  tf.Tensor(3.1142104, shape=(), dtype=float32)\n",
            "batch number:  2280 \t encoder loss:  tf.Tensor(1.1434029, shape=(), dtype=float32)\n",
            "batch number:  2290 \t encoder loss:  tf.Tensor(3.3652356, shape=(), dtype=float32)\n",
            "batch number:  2300 \t encoder loss:  tf.Tensor(2.0978851, shape=(), dtype=float32)\n",
            "batch number:  2310 \t encoder loss:  tf.Tensor(3.008154, shape=(), dtype=float32)\n",
            "batch number:  2320 \t encoder loss:  tf.Tensor(1.3504319, shape=(), dtype=float32)\n",
            "batch number:  2330 \t encoder loss:  tf.Tensor(1.2612706, shape=(), dtype=float32)\n",
            "batch number:  2340 \t encoder loss:  tf.Tensor(0.9405523, shape=(), dtype=float32)\n",
            "batch number:  2350 \t encoder loss:  tf.Tensor(1.2977703, shape=(), dtype=float32)\n",
            "batch number:  2360 \t encoder loss:  tf.Tensor(4.381891, shape=(), dtype=float32)\n",
            "batch number:  2370 \t encoder loss:  tf.Tensor(1.1578867, shape=(), dtype=float32)\n",
            "batch number:  2380 \t encoder loss:  tf.Tensor(1.4943035, shape=(), dtype=float32)\n",
            "batch number:  2390 \t encoder loss:  tf.Tensor(0.88787067, shape=(), dtype=float32)\n",
            "batch number:  2400 \t encoder loss:  tf.Tensor(1.1091384, shape=(), dtype=float32)\n",
            "batch number:  2410 \t encoder loss:  tf.Tensor(0.99979603, shape=(), dtype=float32)\n",
            "batch number:  2420 \t encoder loss:  tf.Tensor(1.721204, shape=(), dtype=float32)\n",
            "batch number:  2430 \t encoder loss:  tf.Tensor(1.6010776, shape=(), dtype=float32)\n",
            "batch number:  2440 \t encoder loss:  tf.Tensor(1.3649005, shape=(), dtype=float32)\n",
            "batch number:  2450 \t encoder loss:  tf.Tensor(2.5486734, shape=(), dtype=float32)\n",
            "batch number:  2460 \t encoder loss:  tf.Tensor(1.3691605, shape=(), dtype=float32)\n",
            "batch number:  2470 \t encoder loss:  tf.Tensor(2.435083, shape=(), dtype=float32)\n",
            "batch number:  2480 \t encoder loss:  tf.Tensor(1.3566754, shape=(), dtype=float32)\n",
            "batch number:  2490 \t encoder loss:  tf.Tensor(1.5080817, shape=(), dtype=float32)\n",
            "batch number:  2500 \t encoder loss:  tf.Tensor(1.248853, shape=(), dtype=float32)\n",
            "batch number:  2510 \t encoder loss:  tf.Tensor(0.95778435, shape=(), dtype=float32)\n",
            "batch number:  2520 \t encoder loss:  tf.Tensor(1.2375053, shape=(), dtype=float32)\n",
            "batch number:  2530 \t encoder loss:  tf.Tensor(3.2065988, shape=(), dtype=float32)\n",
            "batch number:  2540 \t encoder loss:  tf.Tensor(3.557227, shape=(), dtype=float32)\n",
            "batch number:  2550 \t encoder loss:  tf.Tensor(1.37536, shape=(), dtype=float32)\n",
            "batch number:  2560 \t encoder loss:  tf.Tensor(2.8033624, shape=(), dtype=float32)\n",
            "batch number:  2570 \t encoder loss:  tf.Tensor(1.0743134, shape=(), dtype=float32)\n",
            "batch number:  2580 \t encoder loss:  tf.Tensor(1.3891032, shape=(), dtype=float32)\n",
            "batch number:  2590 \t encoder loss:  tf.Tensor(2.136178, shape=(), dtype=float32)\n",
            "batch number:  2600 \t encoder loss:  tf.Tensor(2.0932894, shape=(), dtype=float32)\n",
            "batch number:  2610 \t encoder loss:  tf.Tensor(1.0272319, shape=(), dtype=float32)\n",
            "batch number:  2620 \t encoder loss:  tf.Tensor(1.3906122, shape=(), dtype=float32)\n",
            "batch number:  2630 \t encoder loss:  tf.Tensor(2.9609232, shape=(), dtype=float32)\n",
            "batch number:  2640 \t encoder loss:  tf.Tensor(2.2479815, shape=(), dtype=float32)\n",
            "batch number:  2650 \t encoder loss:  tf.Tensor(0.99631584, shape=(), dtype=float32)\n",
            "batch number:  2660 \t encoder loss:  tf.Tensor(1.319696, shape=(), dtype=float32)\n",
            "batch number:  2670 \t encoder loss:  tf.Tensor(2.5493097, shape=(), dtype=float32)\n",
            "batch number:  2680 \t encoder loss:  tf.Tensor(1.9108365, shape=(), dtype=float32)\n",
            "batch number:  2690 \t encoder loss:  tf.Tensor(1.0768709, shape=(), dtype=float32)\n",
            "batch number:  2700 \t encoder loss:  tf.Tensor(1.1822395, shape=(), dtype=float32)\n",
            "batch number:  2710 \t encoder loss:  tf.Tensor(1.4610612, shape=(), dtype=float32)\n",
            "batch number:  2720 \t encoder loss:  tf.Tensor(1.6521317, shape=(), dtype=float32)\n",
            "batch number:  2730 \t encoder loss:  tf.Tensor(0.9017209, shape=(), dtype=float32)\n",
            "batch number:  2740 \t encoder loss:  tf.Tensor(2.800156, shape=(), dtype=float32)\n",
            "batch number:  2750 \t encoder loss:  tf.Tensor(0.9570965, shape=(), dtype=float32)\n",
            "batch number:  2760 \t encoder loss:  tf.Tensor(1.2648188, shape=(), dtype=float32)\n",
            "batch number:  2770 \t encoder loss:  tf.Tensor(0.9555403, shape=(), dtype=float32)\n",
            "batch number:  2780 \t encoder loss:  tf.Tensor(1.2474488, shape=(), dtype=float32)\n",
            "batch number:  2790 \t encoder loss:  tf.Tensor(0.8930301, shape=(), dtype=float32)\n",
            "batch number:  2800 \t encoder loss:  tf.Tensor(0.84758276, shape=(), dtype=float32)\n",
            "batch number:  2810 \t encoder loss:  tf.Tensor(1.0077109, shape=(), dtype=float32)\n",
            "batch number:  2820 \t encoder loss:  tf.Tensor(1.3214707, shape=(), dtype=float32)\n",
            "batch number:  2830 \t encoder loss:  tf.Tensor(0.9117669, shape=(), dtype=float32)\n",
            "batch number:  2840 \t encoder loss:  tf.Tensor(1.1044614, shape=(), dtype=float32)\n",
            "batch number:  2850 \t encoder loss:  tf.Tensor(1.66797, shape=(), dtype=float32)\n",
            "batch number:  2860 \t encoder loss:  tf.Tensor(3.1833696, shape=(), dtype=float32)\n",
            "batch number:  2870 \t encoder loss:  tf.Tensor(1.2892363, shape=(), dtype=float32)\n",
            "batch number:  2880 \t encoder loss:  tf.Tensor(1.593936, shape=(), dtype=float32)\n",
            "batch number:  2890 \t encoder loss:  tf.Tensor(1.6009802, shape=(), dtype=float32)\n",
            "batch number:  2900 \t encoder loss:  tf.Tensor(0.82418776, shape=(), dtype=float32)\n",
            "batch number:  2910 \t encoder loss:  tf.Tensor(1.0301744, shape=(), dtype=float32)\n",
            "batch number:  2920 \t encoder loss:  tf.Tensor(2.021578, shape=(), dtype=float32)\n",
            "batch number:  2930 \t encoder loss:  tf.Tensor(1.2781439, shape=(), dtype=float32)\n",
            "batch number:  2940 \t encoder loss:  tf.Tensor(1.4491934, shape=(), dtype=float32)\n",
            "batch number:  2950 \t encoder loss:  tf.Tensor(2.24865, shape=(), dtype=float32)\n",
            "batch number:  2960 \t encoder loss:  tf.Tensor(1.8289115, shape=(), dtype=float32)\n",
            "batch number:  2970 \t encoder loss:  tf.Tensor(1.9394743, shape=(), dtype=float32)\n",
            "batch number:  2980 \t encoder loss:  tf.Tensor(2.1969619, shape=(), dtype=float32)\n",
            "batch number:  2990 \t encoder loss:  tf.Tensor(3.1125355, shape=(), dtype=float32)\n",
            "batch number:  3000 \t encoder loss:  tf.Tensor(0.7616581, shape=(), dtype=float32)\n",
            "batch number:  3010 \t encoder loss:  tf.Tensor(3.224754, shape=(), dtype=float32)\n",
            "batch number:  3020 \t encoder loss:  tf.Tensor(1.3704878, shape=(), dtype=float32)\n",
            "batch number:  3030 \t encoder loss:  tf.Tensor(1.3225062, shape=(), dtype=float32)\n",
            "batch number:  3040 \t encoder loss:  tf.Tensor(1.413516, shape=(), dtype=float32)\n",
            "batch number:  3050 \t encoder loss:  tf.Tensor(1.2974112, shape=(), dtype=float32)\n",
            "batch number:  3060 \t encoder loss:  tf.Tensor(1.3515848, shape=(), dtype=float32)\n",
            "batch number:  3070 \t encoder loss:  tf.Tensor(2.012346, shape=(), dtype=float32)\n",
            "batch number:  3080 \t encoder loss:  tf.Tensor(0.95373, shape=(), dtype=float32)\n",
            "batch number:  3090 \t encoder loss:  tf.Tensor(2.101771, shape=(), dtype=float32)\n",
            "batch number:  3100 \t encoder loss:  tf.Tensor(1.0888504, shape=(), dtype=float32)\n",
            "batch number:  3110 \t encoder loss:  tf.Tensor(1.416888, shape=(), dtype=float32)\n",
            "batch number:  3120 \t encoder loss:  tf.Tensor(0.9898217, shape=(), dtype=float32)\n",
            "batch number:  3130 \t encoder loss:  tf.Tensor(2.436421, shape=(), dtype=float32)\n",
            "batch number:  3140 \t encoder loss:  tf.Tensor(2.435946, shape=(), dtype=float32)\n",
            "batch number:  3150 \t encoder loss:  tf.Tensor(0.6335771, shape=(), dtype=float32)\n",
            "batch number:  3160 \t encoder loss:  tf.Tensor(1.3926333, shape=(), dtype=float32)\n",
            "batch number:  3170 \t encoder loss:  tf.Tensor(0.8754137, shape=(), dtype=float32)\n",
            "batch number:  3180 \t encoder loss:  tf.Tensor(1.3250102, shape=(), dtype=float32)\n",
            "batch number:  3190 \t encoder loss:  tf.Tensor(1.502142, shape=(), dtype=float32)\n",
            "batch number:  3200 \t encoder loss:  tf.Tensor(3.2147658, shape=(), dtype=float32)\n",
            "batch number:  3210 \t encoder loss:  tf.Tensor(1.4280043, shape=(), dtype=float32)\n",
            "batch number:  3220 \t encoder loss:  tf.Tensor(0.7412113, shape=(), dtype=float32)\n",
            "batch number:  3230 \t encoder loss:  tf.Tensor(3.2461526, shape=(), dtype=float32)\n",
            "batch number:  3240 \t encoder loss:  tf.Tensor(1.5258267, shape=(), dtype=float32)\n",
            "batch number:  3250 \t encoder loss:  tf.Tensor(0.9697688, shape=(), dtype=float32)\n",
            "batch number:  3260 \t encoder loss:  tf.Tensor(0.8452269, shape=(), dtype=float32)\n",
            "batch number:  3270 \t encoder loss:  tf.Tensor(0.5163898, shape=(), dtype=float32)\n",
            "batch number:  3280 \t encoder loss:  tf.Tensor(1.4058855, shape=(), dtype=float32)\n",
            "batch number:  3290 \t encoder loss:  tf.Tensor(3.0002353, shape=(), dtype=float32)\n",
            "batch number:  3300 \t encoder loss:  tf.Tensor(1.6994678, shape=(), dtype=float32)\n",
            "batch number:  3310 \t encoder loss:  tf.Tensor(1.7202712, shape=(), dtype=float32)\n",
            "batch number:  3320 \t encoder loss:  tf.Tensor(1.3218863, shape=(), dtype=float32)\n",
            "batch number:  3330 \t encoder loss:  tf.Tensor(1.1622794, shape=(), dtype=float32)\n",
            "batch number:  3340 \t encoder loss:  tf.Tensor(3.0005777, shape=(), dtype=float32)\n",
            "batch number:  3350 \t encoder loss:  tf.Tensor(1.2158623, shape=(), dtype=float32)\n",
            "batch number:  3360 \t encoder loss:  tf.Tensor(1.6314946, shape=(), dtype=float32)\n",
            "batch number:  3370 \t encoder loss:  tf.Tensor(1.2563182, shape=(), dtype=float32)\n",
            "batch number:  3380 \t encoder loss:  tf.Tensor(1.1713938, shape=(), dtype=float32)\n",
            "batch number:  3390 \t encoder loss:  tf.Tensor(1.2833123, shape=(), dtype=float32)\n",
            "batch number:  3400 \t encoder loss:  tf.Tensor(1.0809242, shape=(), dtype=float32)\n",
            "batch number:  3410 \t encoder loss:  tf.Tensor(0.98799384, shape=(), dtype=float32)\n",
            "batch number:  3420 \t encoder loss:  tf.Tensor(0.79362464, shape=(), dtype=float32)\n",
            "batch number:  3430 \t encoder loss:  tf.Tensor(2.0328174, shape=(), dtype=float32)\n",
            "batch number:  3440 \t encoder loss:  tf.Tensor(1.4837484, shape=(), dtype=float32)\n",
            "batch number:  3450 \t encoder loss:  tf.Tensor(1.4681059, shape=(), dtype=float32)\n",
            "batch number:  3460 \t encoder loss:  tf.Tensor(1.1068618, shape=(), dtype=float32)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_61997/561035493.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_61997/517301750.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m     68\u001b[0m               \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n******************************************* Seq2Seq Model Training Turn *******************************************\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_recommender_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_recommender_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_recommender_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m               \u001b[0;32mwith\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_61997/1377714829.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mt_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mbatch_userID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_itemID\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;31m# fetch a batch of user doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mbatch_userdoc_flattend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "mt_model.train(n_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xniWi24qcZsb"
      },
      "source": [
        "# **Model Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user id: 9405 recommended items [5319 6236 2753  593 3371 5324 6863 6256 4660 6608]\n",
            "user id: 5259 recommended items [5319 6236 2753  593 3371 5324 6863 6256 4660 6608]\n",
            "user id: 4762 recommended items [5319 6236 2753  593 3371 6863 5324 6256 4660 6608]\n",
            "user id: 190 recommended items [5319 6236 2753  593 3371 5324 6863 6256 4660 6608]\n",
            "user id: 2021 recommended items [5319 6236 2753  593 3371 5324 6863 6256 4660 6608]\n",
            "\n",
            "\n",
            " user id : 9405 recommended item id : 5319 \n",
            " explanation :\n",
            " -  Good Housekeeping said that one (Barilla Plus) tastes like \"white pasta,\" while the other (which like Barilla Whole Grain Thin Spaghetti is made with 51% whole wheat) \"earned points for its deliciously chewy texture and mildly nutty flavor.\n",
            " -  Barilla cooks much better and is ready to serve in minutes.Because this is a whole grain product, the goodness of the germ, endosperm, and fiber are all present.\n",
            " -  It offers a better alternative to traditional pastas which often take out some of the healthiest parts of the grain in order to make the taste more ordinary and acceptable.This spaghetti product cooks easily and the final product is stringy and ready for sauce or seasoning without clumping.\n",
            " -  Barilla Whole Grain Thin Spaghetti is a great tasting product that prepares fast and easily.\n",
            "\n",
            "\n",
            " user id : 9405 recommended item id : 6236 \n",
            " explanation :\n",
            " -  Thus, the great benefit of the HONEY MAIN FRESH STACKS product is not really \"freshness,\" but ease in packing an individual serving for school or trips.Now for the good part.\n",
            " -  Yet another reason for this new packaging is likely to prevent breakage, that might occur with the traditional rectangular crackers, during transportation.But a more likely reason is to allow people to take individual servings with them, perhaps on trips, to school, or to work, with the confidence that the crackers will be kept safe, secure, and intact.\n",
            " -  This will definitely allow those of us without lots of graham cracker eaters in the house to have fresh ones on hand.The perfect size for smores, these are packed as squares, or half the large rectangles we are used to.\n",
            " -  And Nabisco's Honey Maid is - after much testing of other manufacturers' competing product - easily the best graham cracker out there.\n",
            "\n",
            "\n",
            " user id : 9405 recommended item id : 2753 \n",
            " explanation :\n",
            " -  It probably doesn't matter; just a personal quirk, I guess.Anyway, this is good stuff: the taste is nice -- very good for sweetich things like carrots or plums, for example (I always have a bowl of very lightly steamed carrots around, and I snack on them, taking one and dipping it into CO, which is good for two reasons: first, it tastes great, and second, carotene is fat-soluble so you need to combine carrots with some sort of fat or it won't be used by the body well).All in all, there isn't much to say -- CO is CO is CO; this kind is no different from other types I've tried.\n",
            " -  ), even when close to 75 degrees - and for that reason, it is much better for use in baking, where a hard fat works much better than a soft one.But another characteristic of coconut oil that I love is its flavor and aroma - when you open the jar and stick your nose down in it, you are enveloped in the beauty of the coconut's best element, its aroma - its almost frightening how beautiful that smell is.\n",
            " -  Healthy fats, like this oil, will not affect insulin and give us much more stable energy that last longer in the body.5) Tastes great.\n",
            " -  This one is easily the best so far.The aroma is wonderful and the texture is much fuller than the other brands.\n",
            "\n",
            "\n",
            " user id : 9405 recommended item id : 593 \n",
            " explanation :\n",
            " -  Dark Magic DECAF is wonderfully strong and bold- quite similar to the regular, which happens to be my favorite kcup of all.\n",
            " -  A light blend should be FULL of a light flavor, and personally I didn't find this to be the case with GM's breakfast blend.Still, reading through the reviews it seems like many people enjoy the taste, and I won't fault them for it - coffee (like wine, cheese and most of gastronomy) is so subjective sometimes that there's no way I'm going to not recommend at least giving this a try to people who typically enjoy light blends.\n",
            " -  I received a couple of these Breakfast Blend K-Cups in a variety pack and decided to give them a shot - I've always been pretty pleased with Green Mountain coffee (several of their other blends are on my Wish List) and with them sitting there in the box  I figured \"why not?\"\n",
            " -  I was underwhelmed at most of them, but there were a few that were good enough to convince me that my Keurig COULD make an exceptional cup of coffee.\n",
            "\n",
            "\n",
            " user id : 9405 recommended item id : 3371 \n",
            " explanation :\n",
            " -  I would recommend this coconut oil to everyone because of it's many health benefits and versatility of use (You can spread it on your toast it tastes so great or cook with it as you would any cooking oil, as well as use it on your skin.\n",
            " -  I combine it with pure organic cold pressed olive oil- my hair loves it.The quality and texture are great.I like the containers- they're plastic jars which makes storing them a breeze.I also use this on my skin sometimes, and it works GREAT.\n",
            " -  I'm so glad I did, too, because this stuff is great and for a number of things, some of which caught me off guard.- Cooking -I enjoy cooking with it now and then, especially with my quick-fix quesadillas when I don't have time to cook anything else.\n",
            " -  The smell is strong, if you like coconut you'll love this.\n",
            "\n",
            "\n",
            " user id : 9405 recommended item id : 5324 \n",
            " explanation :\n",
            " -  Then when cool enough, I added the remaining ingredients.My nutritious lunch was discussed by a nutritionist, praised by the hikers, and enjoyed by all.\n",
            " -  I love this shape more than spaghetti or regular size penne for any dish that has a rich red sauce.\n",
            " -  I prepared some cooked to al dente perfection with a homemade sauce consisting of Parmesan cheese, milk, butter (unsalted), garlic, basil (dried from last year's garden) and fresh ground black pepper (I try to watch my sodium intake so I let everyone add salt at the table if they want), and it was a hit.Barilla is my go-to pasta brand when I shop for groceries.\n",
            " -  I chose to use the Mezze Penne n.369 to make a turkey pasta salad as part of a nutritious, energy boosting lunch for my charity hike team.Barilla, established in Parma Italy during 1877, is Italy's #1 brand of pasta.\n",
            "\n",
            "\n",
            " user id : 9405 recommended item id : 6863 \n",
            " explanation :\n",
            " -  I am VERY appreciative of the fact that Baby Gourmet makes a tasty product even adults can enjoy and further, that their label is ACCURATE by weight.As has been pointed out by fellow Vine Program member, Work of Life,  Happy Baby Organic Baby Food isn't quite so honest about their content.\n",
            " -  Intended for babies age 6 months and younger, this food offers infants the same natural food goodness that adults have been enjoying thanks to the organic/green movement.This baby food is loaded with vitamins for an infant's good health and well- being.\n",
            " -  Like  Vine Member, Work To Life, I highly rcommend the following book:Feeding Baby Green: The Earth Friendly Program for Healthy, Safe Nutrition During Pregnancy, Childhood, and Beyondand I also recommend the follow-up book by the same author,Raising Baby Green: The Earth-Friendly Guide to Pregnancy, Childbirth, and Baby Careto any of you who are striving to raise healthy children in a grocery store amnnd marketing world full of deceptive labels and advertising.\n",
            " -  In way way, that is good news, however, since that means the product gives your child 4% of the needed proten for the RDA (hopefully the rest is coming from breast milk or an approriate formula) 50% of the RDA for Vitamin A, 140% of the RDA for Vitamin C, no calicium to speak of even though spinach is supposed to be a good source (tells ya something) and 4 % of of the RDA for Iron.The biggest concern I have with this product is the packaging since that big plastic cap looks like a toy and if your child is clever enough to get it off the pouch once emptied, you know it's going stright into their mouth where it woull be chewed upon and possible choked upon.\n",
            "\n",
            "\n",
            " user id : 9405 recommended item id : 6256 \n",
            " explanation :\n",
            " -  This is truly a pasta that you can time to yield a perfect al dente firmness, which is important in soups because they tend to get reheated a number of times.In the customer-supplied images at the top of this page I've provided a photo of how my own recipe for Pasta e fagioli turns out.\n",
            " -  There are lots of great sounding recipes for ditalini to be found online that I'm eager to try (with names like Garlic and Leek Ditalini, Chicken Rollatini with Ditalini, and Lentils with Sopressata and Ditalini) and one can of course use their imagination.Barilla is my go-to pasta brand when I shop for groceries.\n",
            " -  There's a nice recipe on the box, Barilla Ditilini Soup with Barley and Potato that sounds scrumptious and has a prep time of only 15 minutes, 45 minutes to the table.This Barilla Ditalini will cook will cook up \"Al Dente\" in 10 - 11 minutes or if you want more tender pasta, cook an additional minute or two.\n",
            " -  Ditalini is a new pasta shape to me, and because I love to make big batches of homemade soup I was eager to try it (given that Amazon calls it Barilla Ditalini Soup Cuts even though the term \"soup cuts\" does not appear on the package, or at least the package I got).\n",
            "\n",
            "\n",
            " user id : 9405 recommended item id : 4660 \n",
            " explanation :\n",
            " -  I'm glad the apple pieces are small, because they are REALLY sweet, and would overwhelm the mix if they were larger.Taken together, the ingredients in Sahale Classic Fruit + Nut Blend Trail Mix taste great to me.\n",
            " -  I would recommend NOT buying this in bulk (like I had the opportunity to do, more or less) unless you know you and/or your family REALLY enjoys/loves/favours this tasty treat.\n",
            " -  Like potato chips, these almonds can be addictive!Delicious but caloric snack -- if you're not counting calories, these are a great buy.\n",
            " -  This favor combination has quickly become a favorite.Both of these snack mixes are really good, with just the right balance of sweet and savory, crunchy and chewy, and nuts to fruit.\n",
            "\n",
            "\n",
            " user id : 9405 recommended item id : 6608 \n",
            " explanation :\n",
            " -  But these little pasta cylinders were actually very nice, and super easy to use.\n",
            " -  I was afraid they might dissolve completely, but they held up well and added a nice dimesion.Penne Plus (4 stars):  This is like the premium choice in Barilla, and so it is more expensive.\n",
            " -  None of the pieces stuck together, and had no problems twirling the pasta into the bowls I was serving dinner in.Dinner turned out well, and in a vote of confidence my other half commented on the linguine stating \"Well, it tastes like pasta\".\n",
            " -  But we tend to like this (angel hair is one of our top choices), so this was tasty enough.Oven Ready Lasagna (4 stars): I don't do lasagna very often and never from scratch!\n",
            "\n",
            "\n",
            " user id : 5259 recommended item id : 5319 \n",
            " explanation :\n",
            " -  Good Housekeeping said that one (Barilla Plus) tastes like \"white pasta,\" while the other (which like Barilla Whole Grain Thin Spaghetti is made with 51% whole wheat) \"earned points for its deliciously chewy texture and mildly nutty flavor.\n",
            " -  Barilla cooks much better and is ready to serve in minutes.Because this is a whole grain product, the goodness of the germ, endosperm, and fiber are all present.\n",
            " -  It offers a better alternative to traditional pastas which often take out some of the healthiest parts of the grain in order to make the taste more ordinary and acceptable.This spaghetti product cooks easily and the final product is stringy and ready for sauce or seasoning without clumping.\n",
            " -  Barilla Whole Grain Thin Spaghetti is a great tasting product that prepares fast and easily.\n",
            "\n",
            "\n",
            " user id : 5259 recommended item id : 6236 \n",
            " explanation :\n",
            " -  Thus, the great benefit of the HONEY MAIN FRESH STACKS product is not really \"freshness,\" but ease in packing an individual serving for school or trips.Now for the good part.\n",
            " -  Yet another reason for this new packaging is likely to prevent breakage, that might occur with the traditional rectangular crackers, during transportation.But a more likely reason is to allow people to take individual servings with them, perhaps on trips, to school, or to work, with the confidence that the crackers will be kept safe, secure, and intact.\n",
            " -  This will definitely allow those of us without lots of graham cracker eaters in the house to have fresh ones on hand.The perfect size for smores, these are packed as squares, or half the large rectangles we are used to.\n",
            " -  And Nabisco's Honey Maid is - after much testing of other manufacturers' competing product - easily the best graham cracker out there.\n",
            "\n",
            "\n",
            " user id : 5259 recommended item id : 2753 \n",
            " explanation :\n",
            " -  It probably doesn't matter; just a personal quirk, I guess.Anyway, this is good stuff: the taste is nice -- very good for sweetich things like carrots or plums, for example (I always have a bowl of very lightly steamed carrots around, and I snack on them, taking one and dipping it into CO, which is good for two reasons: first, it tastes great, and second, carotene is fat-soluble so you need to combine carrots with some sort of fat or it won't be used by the body well).All in all, there isn't much to say -- CO is CO is CO; this kind is no different from other types I've tried.\n",
            " -  ), even when close to 75 degrees - and for that reason, it is much better for use in baking, where a hard fat works much better than a soft one.But another characteristic of coconut oil that I love is its flavor and aroma - when you open the jar and stick your nose down in it, you are enveloped in the beauty of the coconut's best element, its aroma - its almost frightening how beautiful that smell is.\n",
            " -  Healthy fats, like this oil, will not affect insulin and give us much more stable energy that last longer in the body.5) Tastes great.\n",
            " -  This one is easily the best so far.The aroma is wonderful and the texture is much fuller than the other brands.\n",
            "\n",
            "\n",
            " user id : 5259 recommended item id : 593 \n",
            " explanation :\n",
            " -  Dark Magic DECAF is wonderfully strong and bold- quite similar to the regular, which happens to be my favorite kcup of all.\n",
            " -  A light blend should be FULL of a light flavor, and personally I didn't find this to be the case with GM's breakfast blend.Still, reading through the reviews it seems like many people enjoy the taste, and I won't fault them for it - coffee (like wine, cheese and most of gastronomy) is so subjective sometimes that there's no way I'm going to not recommend at least giving this a try to people who typically enjoy light blends.\n",
            " -  I received a couple of these Breakfast Blend K-Cups in a variety pack and decided to give them a shot - I've always been pretty pleased with Green Mountain coffee (several of their other blends are on my Wish List) and with them sitting there in the box  I figured \"why not?\"\n",
            " -  I was underwhelmed at most of them, but there were a few that were good enough to convince me that my Keurig COULD make an exceptional cup of coffee.\n",
            "\n",
            "\n",
            " user id : 5259 recommended item id : 3371 \n",
            " explanation :\n",
            " -  I would recommend this coconut oil to everyone because of it's many health benefits and versatility of use (You can spread it on your toast it tastes so great or cook with it as you would any cooking oil, as well as use it on your skin.\n",
            " -  I combine it with pure organic cold pressed olive oil- my hair loves it.The quality and texture are great.I like the containers- they're plastic jars which makes storing them a breeze.I also use this on my skin sometimes, and it works GREAT.\n",
            " -  I'm so glad I did, too, because this stuff is great and for a number of things, some of which caught me off guard.- Cooking -I enjoy cooking with it now and then, especially with my quick-fix quesadillas when I don't have time to cook anything else.\n",
            " -  The smell is strong, if you like coconut you'll love this.\n",
            "\n",
            "\n",
            " user id : 5259 recommended item id : 5324 \n",
            " explanation :\n",
            " -  Then when cool enough, I added the remaining ingredients.My nutritious lunch was discussed by a nutritionist, praised by the hikers, and enjoyed by all.\n",
            " -  I love this shape more than spaghetti or regular size penne for any dish that has a rich red sauce.\n",
            " -  I prepared some cooked to al dente perfection with a homemade sauce consisting of Parmesan cheese, milk, butter (unsalted), garlic, basil (dried from last year's garden) and fresh ground black pepper (I try to watch my sodium intake so I let everyone add salt at the table if they want), and it was a hit.Barilla is my go-to pasta brand when I shop for groceries.\n",
            " -  I chose to use the Mezze Penne n.369 to make a turkey pasta salad as part of a nutritious, energy boosting lunch for my charity hike team.Barilla, established in Parma Italy during 1877, is Italy's #1 brand of pasta.\n",
            "\n",
            "\n",
            " user id : 5259 recommended item id : 6863 \n",
            " explanation :\n",
            " -  I am VERY appreciative of the fact that Baby Gourmet makes a tasty product even adults can enjoy and further, that their label is ACCURATE by weight.As has been pointed out by fellow Vine Program member, Work of Life,  Happy Baby Organic Baby Food isn't quite so honest about their content.\n",
            " -  Intended for babies age 6 months and younger, this food offers infants the same natural food goodness that adults have been enjoying thanks to the organic/green movement.This baby food is loaded with vitamins for an infant's good health and well- being.\n",
            " -  Like  Vine Member, Work To Life, I highly rcommend the following book:Feeding Baby Green: The Earth Friendly Program for Healthy, Safe Nutrition During Pregnancy, Childhood, and Beyondand I also recommend the follow-up book by the same author,Raising Baby Green: The Earth-Friendly Guide to Pregnancy, Childbirth, and Baby Careto any of you who are striving to raise healthy children in a grocery store amnnd marketing world full of deceptive labels and advertising.\n",
            " -  In way way, that is good news, however, since that means the product gives your child 4% of the needed proten for the RDA (hopefully the rest is coming from breast milk or an approriate formula) 50% of the RDA for Vitamin A, 140% of the RDA for Vitamin C, no calicium to speak of even though spinach is supposed to be a good source (tells ya something) and 4 % of of the RDA for Iron.The biggest concern I have with this product is the packaging since that big plastic cap looks like a toy and if your child is clever enough to get it off the pouch once emptied, you know it's going stright into their mouth where it woull be chewed upon and possible choked upon.\n",
            "\n",
            "\n",
            " user id : 5259 recommended item id : 6256 \n",
            " explanation :\n",
            " -  This is truly a pasta that you can time to yield a perfect al dente firmness, which is important in soups because they tend to get reheated a number of times.In the customer-supplied images at the top of this page I've provided a photo of how my own recipe for Pasta e fagioli turns out.\n",
            " -  There are lots of great sounding recipes for ditalini to be found online that I'm eager to try (with names like Garlic and Leek Ditalini, Chicken Rollatini with Ditalini, and Lentils with Sopressata and Ditalini) and one can of course use their imagination.Barilla is my go-to pasta brand when I shop for groceries.\n",
            " -  There's a nice recipe on the box, Barilla Ditilini Soup with Barley and Potato that sounds scrumptious and has a prep time of only 15 minutes, 45 minutes to the table.This Barilla Ditalini will cook will cook up \"Al Dente\" in 10 - 11 minutes or if you want more tender pasta, cook an additional minute or two.\n",
            " -  Ditalini is a new pasta shape to me, and because I love to make big batches of homemade soup I was eager to try it (given that Amazon calls it Barilla Ditalini Soup Cuts even though the term \"soup cuts\" does not appear on the package, or at least the package I got).\n",
            "\n",
            "\n",
            " user id : 5259 recommended item id : 4660 \n",
            " explanation :\n",
            " -  I'm glad the apple pieces are small, because they are REALLY sweet, and would overwhelm the mix if they were larger.Taken together, the ingredients in Sahale Classic Fruit + Nut Blend Trail Mix taste great to me.\n",
            " -  I would recommend NOT buying this in bulk (like I had the opportunity to do, more or less) unless you know you and/or your family REALLY enjoys/loves/favours this tasty treat.\n",
            " -  Like potato chips, these almonds can be addictive!Delicious but caloric snack -- if you're not counting calories, these are a great buy.\n",
            " -  This favor combination has quickly become a favorite.Both of these snack mixes are really good, with just the right balance of sweet and savory, crunchy and chewy, and nuts to fruit.\n",
            "\n",
            "\n",
            " user id : 5259 recommended item id : 6608 \n",
            " explanation :\n",
            " -  But these little pasta cylinders were actually very nice, and super easy to use.\n",
            " -  I was afraid they might dissolve completely, but they held up well and added a nice dimesion.Penne Plus (4 stars):  This is like the premium choice in Barilla, and so it is more expensive.\n",
            " -  None of the pieces stuck together, and had no problems twirling the pasta into the bowls I was serving dinner in.Dinner turned out well, and in a vote of confidence my other half commented on the linguine stating \"Well, it tastes like pasta\".\n",
            " -  But we tend to like this (angel hair is one of our top choices), so this was tasty enough.Oven Ready Lasagna (4 stars): I don't do lasagna very often and never from scratch!\n",
            "\n",
            "\n",
            " user id : 4762 recommended item id : 5319 \n",
            " explanation :\n",
            " -  Good Housekeeping said that one (Barilla Plus) tastes like \"white pasta,\" while the other (which like Barilla Whole Grain Thin Spaghetti is made with 51% whole wheat) \"earned points for its deliciously chewy texture and mildly nutty flavor.\n",
            " -  Barilla cooks much better and is ready to serve in minutes.Because this is a whole grain product, the goodness of the germ, endosperm, and fiber are all present.\n",
            " -  It offers a better alternative to traditional pastas which often take out some of the healthiest parts of the grain in order to make the taste more ordinary and acceptable.This spaghetti product cooks easily and the final product is stringy and ready for sauce or seasoning without clumping.\n",
            " -  Barilla Whole Grain Thin Spaghetti is a great tasting product that prepares fast and easily.\n",
            "\n",
            "\n",
            " user id : 4762 recommended item id : 6236 \n",
            " explanation :\n",
            " -  Thus, the great benefit of the HONEY MAIN FRESH STACKS product is not really \"freshness,\" but ease in packing an individual serving for school or trips.Now for the good part.\n",
            " -  Yet another reason for this new packaging is likely to prevent breakage, that might occur with the traditional rectangular crackers, during transportation.But a more likely reason is to allow people to take individual servings with them, perhaps on trips, to school, or to work, with the confidence that the crackers will be kept safe, secure, and intact.\n",
            " -  This will definitely allow those of us without lots of graham cracker eaters in the house to have fresh ones on hand.The perfect size for smores, these are packed as squares, or half the large rectangles we are used to.\n",
            " -  And Nabisco's Honey Maid is - after much testing of other manufacturers' competing product - easily the best graham cracker out there.\n",
            "\n",
            "\n",
            " user id : 4762 recommended item id : 2753 \n",
            " explanation :\n",
            " -  It probably doesn't matter; just a personal quirk, I guess.Anyway, this is good stuff: the taste is nice -- very good for sweetich things like carrots or plums, for example (I always have a bowl of very lightly steamed carrots around, and I snack on them, taking one and dipping it into CO, which is good for two reasons: first, it tastes great, and second, carotene is fat-soluble so you need to combine carrots with some sort of fat or it won't be used by the body well).All in all, there isn't much to say -- CO is CO is CO; this kind is no different from other types I've tried.\n",
            " -  ), even when close to 75 degrees - and for that reason, it is much better for use in baking, where a hard fat works much better than a soft one.But another characteristic of coconut oil that I love is its flavor and aroma - when you open the jar and stick your nose down in it, you are enveloped in the beauty of the coconut's best element, its aroma - its almost frightening how beautiful that smell is.\n",
            " -  Healthy fats, like this oil, will not affect insulin and give us much more stable energy that last longer in the body.5) Tastes great.\n",
            " -  This one is easily the best so far.The aroma is wonderful and the texture is much fuller than the other brands.\n",
            "\n",
            "\n",
            " user id : 4762 recommended item id : 593 \n",
            " explanation :\n",
            " -  Dark Magic DECAF is wonderfully strong and bold- quite similar to the regular, which happens to be my favorite kcup of all.\n",
            " -  A light blend should be FULL of a light flavor, and personally I didn't find this to be the case with GM's breakfast blend.Still, reading through the reviews it seems like many people enjoy the taste, and I won't fault them for it - coffee (like wine, cheese and most of gastronomy) is so subjective sometimes that there's no way I'm going to not recommend at least giving this a try to people who typically enjoy light blends.\n",
            " -  I received a couple of these Breakfast Blend K-Cups in a variety pack and decided to give them a shot - I've always been pretty pleased with Green Mountain coffee (several of their other blends are on my Wish List) and with them sitting there in the box  I figured \"why not?\"\n",
            " -  I was underwhelmed at most of them, but there were a few that were good enough to convince me that my Keurig COULD make an exceptional cup of coffee.\n",
            "\n",
            "\n",
            " user id : 4762 recommended item id : 3371 \n",
            " explanation :\n",
            " -  I would recommend this coconut oil to everyone because of it's many health benefits and versatility of use (You can spread it on your toast it tastes so great or cook with it as you would any cooking oil, as well as use it on your skin.\n",
            " -  I combine it with pure organic cold pressed olive oil- my hair loves it.The quality and texture are great.I like the containers- they're plastic jars which makes storing them a breeze.I also use this on my skin sometimes, and it works GREAT.\n",
            " -  I'm so glad I did, too, because this stuff is great and for a number of things, some of which caught me off guard.- Cooking -I enjoy cooking with it now and then, especially with my quick-fix quesadillas when I don't have time to cook anything else.\n",
            " -  The smell is strong, if you like coconut you'll love this.\n",
            "\n",
            "\n",
            " user id : 4762 recommended item id : 6863 \n",
            " explanation :\n",
            " -  I am VERY appreciative of the fact that Baby Gourmet makes a tasty product even adults can enjoy and further, that their label is ACCURATE by weight.As has been pointed out by fellow Vine Program member, Work of Life,  Happy Baby Organic Baby Food isn't quite so honest about their content.\n",
            " -  Intended for babies age 6 months and younger, this food offers infants the same natural food goodness that adults have been enjoying thanks to the organic/green movement.This baby food is loaded with vitamins for an infant's good health and well- being.\n",
            " -  Like  Vine Member, Work To Life, I highly rcommend the following book:Feeding Baby Green: The Earth Friendly Program for Healthy, Safe Nutrition During Pregnancy, Childhood, and Beyondand I also recommend the follow-up book by the same author,Raising Baby Green: The Earth-Friendly Guide to Pregnancy, Childbirth, and Baby Careto any of you who are striving to raise healthy children in a grocery store amnnd marketing world full of deceptive labels and advertising.\n",
            " -  In way way, that is good news, however, since that means the product gives your child 4% of the needed proten for the RDA (hopefully the rest is coming from breast milk or an approriate formula) 50% of the RDA for Vitamin A, 140% of the RDA for Vitamin C, no calicium to speak of even though spinach is supposed to be a good source (tells ya something) and 4 % of of the RDA for Iron.The biggest concern I have with this product is the packaging since that big plastic cap looks like a toy and if your child is clever enough to get it off the pouch once emptied, you know it's going stright into their mouth where it woull be chewed upon and possible choked upon.\n",
            "\n",
            "\n",
            " user id : 4762 recommended item id : 5324 \n",
            " explanation :\n",
            " -  Then when cool enough, I added the remaining ingredients.My nutritious lunch was discussed by a nutritionist, praised by the hikers, and enjoyed by all.\n",
            " -  I love this shape more than spaghetti or regular size penne for any dish that has a rich red sauce.\n",
            " -  I prepared some cooked to al dente perfection with a homemade sauce consisting of Parmesan cheese, milk, butter (unsalted), garlic, basil (dried from last year's garden) and fresh ground black pepper (I try to watch my sodium intake so I let everyone add salt at the table if they want), and it was a hit.Barilla is my go-to pasta brand when I shop for groceries.\n",
            " -  I chose to use the Mezze Penne n.369 to make a turkey pasta salad as part of a nutritious, energy boosting lunch for my charity hike team.Barilla, established in Parma Italy during 1877, is Italy's #1 brand of pasta.\n",
            "\n",
            "\n",
            " user id : 4762 recommended item id : 6256 \n",
            " explanation :\n",
            " -  This is truly a pasta that you can time to yield a perfect al dente firmness, which is important in soups because they tend to get reheated a number of times.In the customer-supplied images at the top of this page I've provided a photo of how my own recipe for Pasta e fagioli turns out.\n",
            " -  There are lots of great sounding recipes for ditalini to be found online that I'm eager to try (with names like Garlic and Leek Ditalini, Chicken Rollatini with Ditalini, and Lentils with Sopressata and Ditalini) and one can of course use their imagination.Barilla is my go-to pasta brand when I shop for groceries.\n",
            " -  There's a nice recipe on the box, Barilla Ditilini Soup with Barley and Potato that sounds scrumptious and has a prep time of only 15 minutes, 45 minutes to the table.This Barilla Ditalini will cook will cook up \"Al Dente\" in 10 - 11 minutes or if you want more tender pasta, cook an additional minute or two.\n",
            " -  Ditalini is a new pasta shape to me, and because I love to make big batches of homemade soup I was eager to try it (given that Amazon calls it Barilla Ditalini Soup Cuts even though the term \"soup cuts\" does not appear on the package, or at least the package I got).\n",
            "\n",
            "\n",
            " user id : 4762 recommended item id : 4660 \n",
            " explanation :\n",
            " -  I'm glad the apple pieces are small, because they are REALLY sweet, and would overwhelm the mix if they were larger.Taken together, the ingredients in Sahale Classic Fruit + Nut Blend Trail Mix taste great to me.\n",
            " -  I would recommend NOT buying this in bulk (like I had the opportunity to do, more or less) unless you know you and/or your family REALLY enjoys/loves/favours this tasty treat.\n",
            " -  Like potato chips, these almonds can be addictive!Delicious but caloric snack -- if you're not counting calories, these are a great buy.\n",
            " -  This favor combination has quickly become a favorite.Both of these snack mixes are really good, with just the right balance of sweet and savory, crunchy and chewy, and nuts to fruit.\n",
            "\n",
            "\n",
            " user id : 4762 recommended item id : 6608 \n",
            " explanation :\n",
            " -  But these little pasta cylinders were actually very nice, and super easy to use.\n",
            " -  I was afraid they might dissolve completely, but they held up well and added a nice dimesion.Penne Plus (4 stars):  This is like the premium choice in Barilla, and so it is more expensive.\n",
            " -  None of the pieces stuck together, and had no problems twirling the pasta into the bowls I was serving dinner in.Dinner turned out well, and in a vote of confidence my other half commented on the linguine stating \"Well, it tastes like pasta\".\n",
            " -  But we tend to like this (angel hair is one of our top choices), so this was tasty enough.Oven Ready Lasagna (4 stars): I don't do lasagna very often and never from scratch!\n",
            "\n",
            "\n",
            " user id : 190 recommended item id : 5319 \n",
            " explanation :\n",
            " -  Good Housekeeping said that one (Barilla Plus) tastes like \"white pasta,\" while the other (which like Barilla Whole Grain Thin Spaghetti is made with 51% whole wheat) \"earned points for its deliciously chewy texture and mildly nutty flavor.\n",
            " -  Barilla cooks much better and is ready to serve in minutes.Because this is a whole grain product, the goodness of the germ, endosperm, and fiber are all present.\n",
            " -  It offers a better alternative to traditional pastas which often take out some of the healthiest parts of the grain in order to make the taste more ordinary and acceptable.This spaghetti product cooks easily and the final product is stringy and ready for sauce or seasoning without clumping.\n",
            " -  Barilla Whole Grain Thin Spaghetti is a great tasting product that prepares fast and easily.\n",
            "\n",
            "\n",
            " user id : 190 recommended item id : 6236 \n",
            " explanation :\n",
            " -  Thus, the great benefit of the HONEY MAIN FRESH STACKS product is not really \"freshness,\" but ease in packing an individual serving for school or trips.Now for the good part.\n",
            " -  Yet another reason for this new packaging is likely to prevent breakage, that might occur with the traditional rectangular crackers, during transportation.But a more likely reason is to allow people to take individual servings with them, perhaps on trips, to school, or to work, with the confidence that the crackers will be kept safe, secure, and intact.\n",
            " -  This will definitely allow those of us without lots of graham cracker eaters in the house to have fresh ones on hand.The perfect size for smores, these are packed as squares, or half the large rectangles we are used to.\n",
            " -  And Nabisco's Honey Maid is - after much testing of other manufacturers' competing product - easily the best graham cracker out there.\n",
            "\n",
            "\n",
            " user id : 190 recommended item id : 2753 \n",
            " explanation :\n",
            " -  It probably doesn't matter; just a personal quirk, I guess.Anyway, this is good stuff: the taste is nice -- very good for sweetich things like carrots or plums, for example (I always have a bowl of very lightly steamed carrots around, and I snack on them, taking one and dipping it into CO, which is good for two reasons: first, it tastes great, and second, carotene is fat-soluble so you need to combine carrots with some sort of fat or it won't be used by the body well).All in all, there isn't much to say -- CO is CO is CO; this kind is no different from other types I've tried.\n",
            " -  ), even when close to 75 degrees - and for that reason, it is much better for use in baking, where a hard fat works much better than a soft one.But another characteristic of coconut oil that I love is its flavor and aroma - when you open the jar and stick your nose down in it, you are enveloped in the beauty of the coconut's best element, its aroma - its almost frightening how beautiful that smell is.\n",
            " -  Healthy fats, like this oil, will not affect insulin and give us much more stable energy that last longer in the body.5) Tastes great.\n",
            " -  This one is easily the best so far.The aroma is wonderful and the texture is much fuller than the other brands.\n",
            "\n",
            "\n",
            " user id : 190 recommended item id : 593 \n",
            " explanation :\n",
            " -  Dark Magic DECAF is wonderfully strong and bold- quite similar to the regular, which happens to be my favorite kcup of all.\n",
            " -  A light blend should be FULL of a light flavor, and personally I didn't find this to be the case with GM's breakfast blend.Still, reading through the reviews it seems like many people enjoy the taste, and I won't fault them for it - coffee (like wine, cheese and most of gastronomy) is so subjective sometimes that there's no way I'm going to not recommend at least giving this a try to people who typically enjoy light blends.\n",
            " -  I received a couple of these Breakfast Blend K-Cups in a variety pack and decided to give them a shot - I've always been pretty pleased with Green Mountain coffee (several of their other blends are on my Wish List) and with them sitting there in the box  I figured \"why not?\"\n",
            " -  I was underwhelmed at most of them, but there were a few that were good enough to convince me that my Keurig COULD make an exceptional cup of coffee.\n",
            "\n",
            "\n",
            " user id : 190 recommended item id : 3371 \n",
            " explanation :\n",
            " -  I would recommend this coconut oil to everyone because of it's many health benefits and versatility of use (You can spread it on your toast it tastes so great or cook with it as you would any cooking oil, as well as use it on your skin.\n",
            " -  I combine it with pure organic cold pressed olive oil- my hair loves it.The quality and texture are great.I like the containers- they're plastic jars which makes storing them a breeze.I also use this on my skin sometimes, and it works GREAT.\n",
            " -  I'm so glad I did, too, because this stuff is great and for a number of things, some of which caught me off guard.- Cooking -I enjoy cooking with it now and then, especially with my quick-fix quesadillas when I don't have time to cook anything else.\n",
            " -  The smell is strong, if you like coconut you'll love this.\n",
            "\n",
            "\n",
            " user id : 190 recommended item id : 5324 \n",
            " explanation :\n",
            " -  Then when cool enough, I added the remaining ingredients.My nutritious lunch was discussed by a nutritionist, praised by the hikers, and enjoyed by all.\n",
            " -  I love this shape more than spaghetti or regular size penne for any dish that has a rich red sauce.\n",
            " -  I prepared some cooked to al dente perfection with a homemade sauce consisting of Parmesan cheese, milk, butter (unsalted), garlic, basil (dried from last year's garden) and fresh ground black pepper (I try to watch my sodium intake so I let everyone add salt at the table if they want), and it was a hit.Barilla is my go-to pasta brand when I shop for groceries.\n",
            " -  I chose to use the Mezze Penne n.369 to make a turkey pasta salad as part of a nutritious, energy boosting lunch for my charity hike team.Barilla, established in Parma Italy during 1877, is Italy's #1 brand of pasta.\n",
            "\n",
            "\n",
            " user id : 190 recommended item id : 6863 \n",
            " explanation :\n",
            " -  I am VERY appreciative of the fact that Baby Gourmet makes a tasty product even adults can enjoy and further, that their label is ACCURATE by weight.As has been pointed out by fellow Vine Program member, Work of Life,  Happy Baby Organic Baby Food isn't quite so honest about their content.\n",
            " -  Intended for babies age 6 months and younger, this food offers infants the same natural food goodness that adults have been enjoying thanks to the organic/green movement.This baby food is loaded with vitamins for an infant's good health and well- being.\n",
            " -  Like  Vine Member, Work To Life, I highly rcommend the following book:Feeding Baby Green: The Earth Friendly Program for Healthy, Safe Nutrition During Pregnancy, Childhood, and Beyondand I also recommend the follow-up book by the same author,Raising Baby Green: The Earth-Friendly Guide to Pregnancy, Childbirth, and Baby Careto any of you who are striving to raise healthy children in a grocery store amnnd marketing world full of deceptive labels and advertising.\n",
            " -  In way way, that is good news, however, since that means the product gives your child 4% of the needed proten for the RDA (hopefully the rest is coming from breast milk or an approriate formula) 50% of the RDA for Vitamin A, 140% of the RDA for Vitamin C, no calicium to speak of even though spinach is supposed to be a good source (tells ya something) and 4 % of of the RDA for Iron.The biggest concern I have with this product is the packaging since that big plastic cap looks like a toy and if your child is clever enough to get it off the pouch once emptied, you know it's going stright into their mouth where it woull be chewed upon and possible choked upon.\n",
            "\n",
            "\n",
            " user id : 190 recommended item id : 6256 \n",
            " explanation :\n",
            " -  This is truly a pasta that you can time to yield a perfect al dente firmness, which is important in soups because they tend to get reheated a number of times.In the customer-supplied images at the top of this page I've provided a photo of how my own recipe for Pasta e fagioli turns out.\n",
            " -  There are lots of great sounding recipes for ditalini to be found online that I'm eager to try (with names like Garlic and Leek Ditalini, Chicken Rollatini with Ditalini, and Lentils with Sopressata and Ditalini) and one can of course use their imagination.Barilla is my go-to pasta brand when I shop for groceries.\n",
            " -  There's a nice recipe on the box, Barilla Ditilini Soup with Barley and Potato that sounds scrumptious and has a prep time of only 15 minutes, 45 minutes to the table.This Barilla Ditalini will cook will cook up \"Al Dente\" in 10 - 11 minutes or if you want more tender pasta, cook an additional minute or two.\n",
            " -  Ditalini is a new pasta shape to me, and because I love to make big batches of homemade soup I was eager to try it (given that Amazon calls it Barilla Ditalini Soup Cuts even though the term \"soup cuts\" does not appear on the package, or at least the package I got).\n",
            "\n",
            "\n",
            " user id : 190 recommended item id : 4660 \n",
            " explanation :\n",
            " -  I'm glad the apple pieces are small, because they are REALLY sweet, and would overwhelm the mix if they were larger.Taken together, the ingredients in Sahale Classic Fruit + Nut Blend Trail Mix taste great to me.\n",
            " -  I would recommend NOT buying this in bulk (like I had the opportunity to do, more or less) unless you know you and/or your family REALLY enjoys/loves/favours this tasty treat.\n",
            " -  Like potato chips, these almonds can be addictive!Delicious but caloric snack -- if you're not counting calories, these are a great buy.\n",
            " -  This favor combination has quickly become a favorite.Both of these snack mixes are really good, with just the right balance of sweet and savory, crunchy and chewy, and nuts to fruit.\n",
            "\n",
            "\n",
            " user id : 190 recommended item id : 6608 \n",
            " explanation :\n",
            " -  But these little pasta cylinders were actually very nice, and super easy to use.\n",
            " -  I was afraid they might dissolve completely, but they held up well and added a nice dimesion.Penne Plus (4 stars):  This is like the premium choice in Barilla, and so it is more expensive.\n",
            " -  None of the pieces stuck together, and had no problems twirling the pasta into the bowls I was serving dinner in.Dinner turned out well, and in a vote of confidence my other half commented on the linguine stating \"Well, it tastes like pasta\".\n",
            " -  But we tend to like this (angel hair is one of our top choices), so this was tasty enough.Oven Ready Lasagna (4 stars): I don't do lasagna very often and never from scratch!\n",
            "\n",
            "\n",
            " user id : 2021 recommended item id : 5319 \n",
            " explanation :\n",
            " -  Good Housekeeping said that one (Barilla Plus) tastes like \"white pasta,\" while the other (which like Barilla Whole Grain Thin Spaghetti is made with 51% whole wheat) \"earned points for its deliciously chewy texture and mildly nutty flavor.\n",
            " -  Barilla cooks much better and is ready to serve in minutes.Because this is a whole grain product, the goodness of the germ, endosperm, and fiber are all present.\n",
            " -  It offers a better alternative to traditional pastas which often take out some of the healthiest parts of the grain in order to make the taste more ordinary and acceptable.This spaghetti product cooks easily and the final product is stringy and ready for sauce or seasoning without clumping.\n",
            " -  Barilla Whole Grain Thin Spaghetti is a great tasting product that prepares fast and easily.\n",
            "\n",
            "\n",
            " user id : 2021 recommended item id : 6236 \n",
            " explanation :\n",
            " -  Thus, the great benefit of the HONEY MAIN FRESH STACKS product is not really \"freshness,\" but ease in packing an individual serving for school or trips.Now for the good part.\n",
            " -  Yet another reason for this new packaging is likely to prevent breakage, that might occur with the traditional rectangular crackers, during transportation.But a more likely reason is to allow people to take individual servings with them, perhaps on trips, to school, or to work, with the confidence that the crackers will be kept safe, secure, and intact.\n",
            " -  This will definitely allow those of us without lots of graham cracker eaters in the house to have fresh ones on hand.The perfect size for smores, these are packed as squares, or half the large rectangles we are used to.\n",
            " -  And Nabisco's Honey Maid is - after much testing of other manufacturers' competing product - easily the best graham cracker out there.\n",
            "\n",
            "\n",
            " user id : 2021 recommended item id : 2753 \n",
            " explanation :\n",
            " -  It probably doesn't matter; just a personal quirk, I guess.Anyway, this is good stuff: the taste is nice -- very good for sweetich things like carrots or plums, for example (I always have a bowl of very lightly steamed carrots around, and I snack on them, taking one and dipping it into CO, which is good for two reasons: first, it tastes great, and second, carotene is fat-soluble so you need to combine carrots with some sort of fat or it won't be used by the body well).All in all, there isn't much to say -- CO is CO is CO; this kind is no different from other types I've tried.\n",
            " -  ), even when close to 75 degrees - and for that reason, it is much better for use in baking, where a hard fat works much better than a soft one.But another characteristic of coconut oil that I love is its flavor and aroma - when you open the jar and stick your nose down in it, you are enveloped in the beauty of the coconut's best element, its aroma - its almost frightening how beautiful that smell is.\n",
            " -  Healthy fats, like this oil, will not affect insulin and give us much more stable energy that last longer in the body.5) Tastes great.\n",
            " -  This one is easily the best so far.The aroma is wonderful and the texture is much fuller than the other brands.\n",
            "\n",
            "\n",
            " user id : 2021 recommended item id : 593 \n",
            " explanation :\n",
            " -  Dark Magic DECAF is wonderfully strong and bold- quite similar to the regular, which happens to be my favorite kcup of all.\n",
            " -  A light blend should be FULL of a light flavor, and personally I didn't find this to be the case with GM's breakfast blend.Still, reading through the reviews it seems like many people enjoy the taste, and I won't fault them for it - coffee (like wine, cheese and most of gastronomy) is so subjective sometimes that there's no way I'm going to not recommend at least giving this a try to people who typically enjoy light blends.\n",
            " -  I received a couple of these Breakfast Blend K-Cups in a variety pack and decided to give them a shot - I've always been pretty pleased with Green Mountain coffee (several of their other blends are on my Wish List) and with them sitting there in the box  I figured \"why not?\"\n",
            " -  I was underwhelmed at most of them, but there were a few that were good enough to convince me that my Keurig COULD make an exceptional cup of coffee.\n",
            "\n",
            "\n",
            " user id : 2021 recommended item id : 3371 \n",
            " explanation :\n",
            " -  I would recommend this coconut oil to everyone because of it's many health benefits and versatility of use (You can spread it on your toast it tastes so great or cook with it as you would any cooking oil, as well as use it on your skin.\n",
            " -  I combine it with pure organic cold pressed olive oil- my hair loves it.The quality and texture are great.I like the containers- they're plastic jars which makes storing them a breeze.I also use this on my skin sometimes, and it works GREAT.\n",
            " -  I'm so glad I did, too, because this stuff is great and for a number of things, some of which caught me off guard.- Cooking -I enjoy cooking with it now and then, especially with my quick-fix quesadillas when I don't have time to cook anything else.\n",
            " -  The smell is strong, if you like coconut you'll love this.\n",
            "\n",
            "\n",
            " user id : 2021 recommended item id : 5324 \n",
            " explanation :\n",
            " -  Then when cool enough, I added the remaining ingredients.My nutritious lunch was discussed by a nutritionist, praised by the hikers, and enjoyed by all.\n",
            " -  I love this shape more than spaghetti or regular size penne for any dish that has a rich red sauce.\n",
            " -  I prepared some cooked to al dente perfection with a homemade sauce consisting of Parmesan cheese, milk, butter (unsalted), garlic, basil (dried from last year's garden) and fresh ground black pepper (I try to watch my sodium intake so I let everyone add salt at the table if they want), and it was a hit.Barilla is my go-to pasta brand when I shop for groceries.\n",
            " -  I chose to use the Mezze Penne n.369 to make a turkey pasta salad as part of a nutritious, energy boosting lunch for my charity hike team.Barilla, established in Parma Italy during 1877, is Italy's #1 brand of pasta.\n",
            "\n",
            "\n",
            " user id : 2021 recommended item id : 6863 \n",
            " explanation :\n",
            " -  I am VERY appreciative of the fact that Baby Gourmet makes a tasty product even adults can enjoy and further, that their label is ACCURATE by weight.As has been pointed out by fellow Vine Program member, Work of Life,  Happy Baby Organic Baby Food isn't quite so honest about their content.\n",
            " -  Intended for babies age 6 months and younger, this food offers infants the same natural food goodness that adults have been enjoying thanks to the organic/green movement.This baby food is loaded with vitamins for an infant's good health and well- being.\n",
            " -  Like  Vine Member, Work To Life, I highly rcommend the following book:Feeding Baby Green: The Earth Friendly Program for Healthy, Safe Nutrition During Pregnancy, Childhood, and Beyondand I also recommend the follow-up book by the same author,Raising Baby Green: The Earth-Friendly Guide to Pregnancy, Childbirth, and Baby Careto any of you who are striving to raise healthy children in a grocery store amnnd marketing world full of deceptive labels and advertising.\n",
            " -  In way way, that is good news, however, since that means the product gives your child 4% of the needed proten for the RDA (hopefully the rest is coming from breast milk or an approriate formula) 50% of the RDA for Vitamin A, 140% of the RDA for Vitamin C, no calicium to speak of even though spinach is supposed to be a good source (tells ya something) and 4 % of of the RDA for Iron.The biggest concern I have with this product is the packaging since that big plastic cap looks like a toy and if your child is clever enough to get it off the pouch once emptied, you know it's going stright into their mouth where it woull be chewed upon and possible choked upon.\n",
            "\n",
            "\n",
            " user id : 2021 recommended item id : 6256 \n",
            " explanation :\n",
            " -  This is truly a pasta that you can time to yield a perfect al dente firmness, which is important in soups because they tend to get reheated a number of times.In the customer-supplied images at the top of this page I've provided a photo of how my own recipe for Pasta e fagioli turns out.\n",
            " -  There are lots of great sounding recipes for ditalini to be found online that I'm eager to try (with names like Garlic and Leek Ditalini, Chicken Rollatini with Ditalini, and Lentils with Sopressata and Ditalini) and one can of course use their imagination.Barilla is my go-to pasta brand when I shop for groceries.\n",
            " -  There's a nice recipe on the box, Barilla Ditilini Soup with Barley and Potato that sounds scrumptious and has a prep time of only 15 minutes, 45 minutes to the table.This Barilla Ditalini will cook will cook up \"Al Dente\" in 10 - 11 minutes or if you want more tender pasta, cook an additional minute or two.\n",
            " -  Ditalini is a new pasta shape to me, and because I love to make big batches of homemade soup I was eager to try it (given that Amazon calls it Barilla Ditalini Soup Cuts even though the term \"soup cuts\" does not appear on the package, or at least the package I got).\n",
            "\n",
            "\n",
            " user id : 2021 recommended item id : 4660 \n",
            " explanation :\n",
            " -  I'm glad the apple pieces are small, because they are REALLY sweet, and would overwhelm the mix if they were larger.Taken together, the ingredients in Sahale Classic Fruit + Nut Blend Trail Mix taste great to me.\n",
            " -  I would recommend NOT buying this in bulk (like I had the opportunity to do, more or less) unless you know you and/or your family REALLY enjoys/loves/favours this tasty treat.\n",
            " -  Like potato chips, these almonds can be addictive!Delicious but caloric snack -- if you're not counting calories, these are a great buy.\n",
            " -  This favor combination has quickly become a favorite.Both of these snack mixes are really good, with just the right balance of sweet and savory, crunchy and chewy, and nuts to fruit.\n",
            "\n",
            "\n",
            " user id : 2021 recommended item id : 6608 \n",
            " explanation :\n",
            " -  But these little pasta cylinders were actually very nice, and super easy to use.\n",
            " -  I was afraid they might dissolve completely, but they held up well and added a nice dimesion.Penne Plus (4 stars):  This is like the premium choice in Barilla, and so it is more expensive.\n",
            " -  None of the pieces stuck together, and had no problems twirling the pasta into the bowls I was serving dinner in.Dinner turned out well, and in a vote of confidence my other half commented on the linguine stating \"Well, it tastes like pasta\".\n",
            " -  But we tend to like this (angel hair is one of our top choices), so this was tasty enough.Oven Ready Lasagna (4 stars): I don't do lasagna very often and never from scratch!\n"
          ]
        }
      ],
      "source": [
        "mt_model.predict(5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Explainable Recommender System.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.13 ('colabenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "5859fc1fd51a29bc96a6c335b5cef2533de774a99a73e1484108bae0d11f06ff"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
