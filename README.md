# A Framework For Generating Explainable Recommendations Via Multitask Learning


## Abstract
Recommendation systems predict desirable recommendations for users based on their past
interactions (e.g., ratings, likes, and shopping history). These systems face challenges such as
data insufficiency and lack of transparency. Leveraging user reviews has been proposed as a
solution to address these challenges. First of all, reviews convey user preferences so that they
can be used as supplementary source of data. Secondly, reviews are a great language source
to generate textual explanations, helping overcome transparency issues. Recently, due to the
potential threats of AI to society, the concept of Responsible AI (RAI) has gained considerable
attention. Fundamentally, RAI addresses ethical and societal concerns. Transparency is known as
one of the primary principles of RAI because it helps end-users to understand the reasons behind
the predictions of AI models (e.g., recommender systems). Therefore, e-commerce platforms
that use recommender systems should provide users with explainable recommendations to act
responsibly. Text is a prevalent style of explanation employed in a wide range of explainable
recommendation systems. However, previous works often neglect the writing style of users,
which can impact the effectiveness of explanations. The main objective of the paper is to develop
a novel framework for generating explainable recommendations that incorporate user writing
styles and preferences in the explanation generation process.


<img width="653" alt="image" src="https://user-images.githubusercontent.com/103757072/233791305-44be5fa7-e794-411e-8863-c7709ec389ef.png">

### Keywords
recommender system- responsible AI - multi-task learning - explanation generation - explainable recommendation - text reconstruction
